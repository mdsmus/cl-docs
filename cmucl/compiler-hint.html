<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
"http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
<head>
<meta name="generator" content=
"HTML Tidy for Mac OS X (vers 31 October 2006 - Apple Inc. build 13), see www.w3.org">
<meta http-equiv="Content-Type" content=
"text/html; charset=us-ascii">
<meta name="GENERATOR" content="hevea 1.06">
<link rel="stylesheet" href="cmucl.css" type="text/css">
<meta http-equiv="Content-Language" content="en">
<title>CMUCL User's Manual: Advanced Compiler Use and Efficiency
Hints</title>
</head>
<body>
<a href="compiler.html"><img src="previous_motif.gif" alt=
"Previous"></a> <a href="index.html"><img src="contents_motif.gif"
alt="Up"></a> <a href="unix.html"><img src="next_motif.gif" alt=
"Next"></a>
<hr>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFBC">
<div align="center">
<table>
<tr>
<td><a name="htoc125" id="htoc125"><b><font size=
"6">Chapter&nbsp;5</font></b></a></td>
<td width="100%" align="center"><b><font size="6">Advanced Compiler
Use and Efficiency Hints</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<ul>
<li><a href="compiler-hint.html#toc133">Advanced Compiler
Introduction</a>
<ul>
<li><a href="compiler-hint.html#toc134">Types</a></li>
<li><a href="compiler-hint.html#toc135">Optimization</a></li>
<li><a href="compiler-hint.html#toc136">Function Call</a></li>
<li><a href="compiler-hint.html#toc137">Representation of
Objects</a></li>
<li><a href="compiler-hint.html#toc138">Writing Efficient
Code</a></li>
</ul>
</li>
<li><a href="compiler-hint.html#toc139">More About Types in
Python</a>
<ul>
<li><a href="compiler-hint.html#toc140">More Types
Meaningful</a></li>
<li><a href="compiler-hint.html#toc141">Canonicalization</a></li>
<li><a href="compiler-hint.html#toc142">Member Types</a></li>
<li><a href="compiler-hint.html#toc143">Union Types</a></li>
<li><a href="compiler-hint.html#toc144">The Empty Type</a></li>
<li><a href="compiler-hint.html#toc145">Function Types</a></li>
<li><a href="compiler-hint.html#toc146">The Values
Declaration</a></li>
<li><a href="compiler-hint.html#toc147">Structure Types</a></li>
<li><a href="compiler-hint.html#toc148">The Freeze-Type
Declaration</a></li>
<li><a href="compiler-hint.html#toc149">Type Restrictions</a></li>
<li><a href="compiler-hint.html#toc150">Type Style
Recommendations</a></li>
</ul>
</li>
<li><a href="compiler-hint.html#toc151">Type Inference</a>
<ul>
<li><a href="compiler-hint.html#toc152">Variable Type
Inference</a></li>
<li><a href="compiler-hint.html#toc153">Local Function Type
Inference</a></li>
<li><a href="compiler-hint.html#toc154">Global Function Type
Inference</a></li>
<li><a href="compiler-hint.html#toc155">Operation Specific Type
Inference</a></li>
<li><a href="compiler-hint.html#toc156">Dynamic Type
Inference</a></li>
<li><a href="compiler-hint.html#toc157">Type Check
Optimization</a></li>
</ul>
</li>
<li><a href="compiler-hint.html#toc158">Source Optimization</a>
<ul>
<li><a href="compiler-hint.html#toc159">Let Optimization</a></li>
<li><a href="compiler-hint.html#toc160">Constant Folding</a></li>
<li><a href="compiler-hint.html#toc161">Unused Expression
Elimination</a></li>
<li><a href="compiler-hint.html#toc162">Control
Optimization</a></li>
<li><a href="compiler-hint.html#toc163">Unreachable Code
Deletion</a></li>
<li><a href="compiler-hint.html#toc164">Multiple Values
Optimization</a></li>
<li><a href="compiler-hint.html#toc165">Source to Source
Transformation</a></li>
<li><a href="compiler-hint.html#toc166">Style
Recommendations</a></li>
</ul>
</li>
<li><a href="compiler-hint.html#toc167">Tail Recursion</a>
<ul>
<li><a href="compiler-hint.html#toc168">Tail Recursion
Exceptions</a></li>
</ul>
</li>
<li><a href="compiler-hint.html#toc169">Local Call</a>
<ul>
<li><a href="compiler-hint.html#toc170">Self-Recursive
Calls</a></li>
<li><a href="compiler-hint.html#toc171">Let Calls</a></li>
<li><a href="compiler-hint.html#toc172">Closures</a></li>
<li><a href="compiler-hint.html#toc173">Local Tail
Recursion</a></li>
<li><a href="compiler-hint.html#toc174">Return Values</a></li>
</ul>
</li>
<li><a href="compiler-hint.html#toc175">Block Compilation</a>
<ul>
<li><a href="compiler-hint.html#toc176">Block Compilation
Semantics</a></li>
<li><a href="compiler-hint.html#toc177">Block Compilation
Declarations</a></li>
<li><a href="compiler-hint.html#toc178">Compiler Arguments</a></li>
<li><a href="compiler-hint.html#toc179">Practical
Difficulties</a></li>
<li><a href="compiler-hint.html#toc180">Context
Declarations</a></li>
<li><a href="compiler-hint.html#toc181">Context Declaration
Example</a></li>
</ul>
</li>
<li><a href="compiler-hint.html#toc182">Inline Expansion</a>
<ul>
<li><a href="compiler-hint.html#toc183">Inline Expansion
Recording</a></li>
<li><a href="compiler-hint.html#toc184">Semi-Inline
Expansion</a></li>
<li><a href="compiler-hint.html#toc185">The Maybe-Inline
Declaration</a></li>
</ul>
</li>
<li><a href="compiler-hint.html#toc186">Byte Coded
Compilation</a></li>
<li><a href="compiler-hint.html#toc187">Object Representation</a>
<ul>
<li><a href="compiler-hint.html#toc188">Think Before You Use a
List</a></li>
<li><a href="compiler-hint.html#toc189">Structure
Representation</a></li>
<li><a href="compiler-hint.html#toc190">Arrays</a></li>
<li><a href="compiler-hint.html#toc191">Vectors</a></li>
<li><a href="compiler-hint.html#toc192">Bit-Vectors</a></li>
<li><a href="compiler-hint.html#toc193">Hashtables</a></li>
</ul>
</li>
<li><a href="compiler-hint.html#toc194">Numbers</a>
<ul>
<li><a href="compiler-hint.html#toc195">Descriptors</a></li>
<li><a href="compiler-hint.html#toc196">Non-Descriptor
Representations</a></li>
<li><a href="compiler-hint.html#toc197">Variables</a></li>
<li><a href="compiler-hint.html#toc198">Generic Arithmetic</a></li>
<li><a href="compiler-hint.html#toc199">Fixnums</a></li>
<li><a href="compiler-hint.html#toc200">Word Integers</a></li>
<li><a href="compiler-hint.html#toc201">Floating Point
Efficiency</a>
<ul>
<li><a href="compiler-hint.html#toc202">Signed Zeroes and Special
Functions</a></li>
</ul>
</li>
<li><a href="compiler-hint.html#toc203">Specialized Arrays</a></li>
<li><a href="compiler-hint.html#toc204">Specialized Structure
Slots</a></li>
<li><a href="compiler-hint.html#toc205">Interactions With Local
Call</a></li>
<li><a href="compiler-hint.html#toc206">Representation of
Characters</a></li>
</ul>
</li>
<li><a href="compiler-hint.html#toc207">General Efficiency
Hints</a>
<ul>
<li><a href="compiler-hint.html#toc208">Compile Your Code</a></li>
<li><a href="compiler-hint.html#toc209">Avoid Unnecessary
Consing</a></li>
<li><a href="compiler-hint.html#toc210">Complex Argument
Syntax</a></li>
<li><a href="compiler-hint.html#toc211">Mapping and
Iteration</a></li>
<li><a href="compiler-hint.html#toc212">Trace Files and
Disassembly</a></li>
</ul>
</li>
<li><a href="compiler-hint.html#toc213">Efficiency Notes</a>
<ul>
<li><a href="compiler-hint.html#toc214">Type Uncertainty</a></li>
<li><a href="compiler-hint.html#toc215">Efficiency Notes and Type
Checking</a></li>
<li><a href="compiler-hint.html#toc216">Representation Efficiency
Notes</a></li>
<li><a href="compiler-hint.html#toc217">Verbosity Control</a></li>
</ul>
</li>
<li><a href="compiler-hint.html#toc218">Profiling</a>
<ul>
<li><a href="compiler-hint.html#toc219">Profile Interface</a></li>
<li><a href="compiler-hint.html#toc220">Profiling
Techniques</a></li>
<li><a href="compiler-hint.html#toc221">Nested or Recursive
Calls</a></li>
<li><a href="compiler-hint.html#toc222">Clock resolution</a></li>
<li><a href="compiler-hint.html#toc223">Profiling overhead</a></li>
<li><a href="compiler-hint.html#toc224">Additional Timing
Utilities</a></li>
<li><a href="compiler-hint.html#toc225">A Note on Timing</a></li>
<li><a href="compiler-hint.html#toc226">Benchmarking
Techniques</a></li>
</ul>
</li>
</ul>
<a name="advanced-compiler" id="advanced-compiler"></a><br>
<div align="center"><b>by Robert MacLachlan</b></div>
<br>
<a name="toc133" id="toc133"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFD3">
<div align="center">
<table>
<tr>
<td><a name="htoc126" id="htoc126"><b><font size=
"5">5.1</font></b></a></td>
<td width="100%" align="center"><b><font size="5">Advanced Compiler
Introduction</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<br>
In CMUCL, as with any language on any computer, the path to
efficient code starts with good algorithms and sensible programming
techniques, but to avoid inefficiency pitfalls, you need to know
some of this implementation's quirks and features. This chapter is
mostly a fairly long and detailed overview of what optimizations
Python does. Although there are the usual negative suggestions of
inefficient features to avoid, the main emphasis is on describing
the things that programmers can count on being efficient.<br>
<br>
The optimizations described here can have the effect of speeding up
existing programs written in conventional styles, but the potential
for new programming styles that are clearer and less error-prone is
at least as significant. For this reason, several sections end with
a discussion of the implications of these optimizations for
programming style.<br>
<br>
<a name="toc134" id="toc134"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc127" id="htoc127"><b><font size=
"4">5.1.1</font></b></a></td>
<td width="100%" align="center"><b><font size=
"4">Types</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<br>
Python's support for types is unusual in three major ways:
<ul>
<li>Precise type checking encourages the specific use of type
declarations as a form of run-time consistency checking. This
speeds development by localizing type errors and giving more
meaningful error messages. See section&nbsp;<a href=
"compiler.html#precise-type-checks">4.5.2</a>. Python produces
completely safe code; optimized type checking maintains reasonable
efficiency on conventional hardware (see section&nbsp;<a href=
"#type-check-optimization">5.3.6</a>.)<br>
<br></li>
<li>Comprehensive support for the Common Lisp type system makes
complex type specifiers useful. Using type specifiers such as
<tt class="code">or</tt> and <tt class="code">member</tt> has both
efficiency and robustness advantages. See section&nbsp;<a href=
"#advanced-type-stuff">5.2</a>.<br>
<br></li>
<li>Type inference eliminates the need for some declarations, and
also aids compile-time detection of type errors. Given detailed
type declarations, type inference can often eliminate type checks
and enable more efficient object representations and code
sequences. Checking all types results in fewer type checks. See
sections <a href="#type-inference">5.3</a> and <a href=
"#non-descriptor">5.11.2</a>.</li>
</ul>
<a name="toc135" id="toc135"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc128" id="htoc128"><b><font size=
"4">5.1.2</font></b></a></td>
<td width="100%" align="center"><b><font size=
"4">Optimization</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<br>
The main barrier to efficient Lisp programs is not that there is no
efficient way to code the program in Lisp, but that it is difficult
to arrive at that efficient coding. Common Lisp is a highly complex
language, and usually has many semantically equivalent
``reasonable'' ways to code a given problem. It is desirable to
make all of these equivalent solutions have comparable efficiency
so that programmers don't have to waste time discovering the most
efficient solution.<br>
<br>
Source level optimization increases the number of efficient ways to
solve a problem. This effect is much larger than the increase in
the efficiency of the ``best'' solution. Source level optimization
transforms the original program into a more efficient (but
equivalent) program. Although the optimizer isn't doing anything
the programmer couldn't have done, this high-level optimization is
important because:
<ul>
<li>The programmer can code simply and directly, rather than
obfuscating code to please the compiler.<br>
<br></li>
<li>When presented with a choice of similar coding alternatives,
the programmer can chose whichever happens to be most convenient,
instead of worrying about which is most efficient.</li>
</ul>
Source level optimization eliminates the need for macros to
optimize their expansion, and also increases the effectiveness of
inline expansion. See sections <a href=
"#source-optimization">5.4</a> and <a href=
"#inline-expansion">5.8</a>.<br>
<br>
Efficient support for a safer programming style is the biggest
advantage of source level optimization. Existing tuned programs
typically won't benefit much from source optimization, since their
source has already been optimized by hand. However, even tuned
programs tend to run faster under Python because:
<ul>
<li>Low level optimization and register allocation provides modest
speedups in any program.<br>
<br></li>
<li>Block compilation and inline expansion can reduce function call
overhead, but may require some program restructuring. See sections
<a href="#inline-expansion">5.8</a>, <a href="#local-call">5.6</a>
and <a href="#block-compilation">5.7</a>.<br>
<br></li>
<li>Efficiency notes will point out important type declarations
that are often missed even in highly tuned programs. See
section&nbsp;<a href="#efficiency-notes">5.13</a>.<br>
<br></li>
<li>Existing programs can be compiled safely without prohibitive
speed penalty, although they would be faster and safer with added
declarations. See section&nbsp;<a href=
"#type-check-optimization">5.3.6</a>.<br>
<br></li>
<li>The context declaration mechanism allows both space and runtime
of large systems to be reduced without sacrificing robustness by
semi-automatically varying compilation policy without addition any
<tt class="code">optimize</tt> declarations to the source. See
section&nbsp;<a href="#context-declarations">5.7.5</a>.<br>
<br></li>
<li>Byte compilation can be used to dramatically reduce the size of
code that is not speed-critical. See section&nbsp;<a href=
"#byte-compile">5.9</a></li>
</ul>
<a name="toc136" id="toc136"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc129" id="htoc129"><b><font size=
"4">5.1.3</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Function
Call</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<br>
The sort of symbolic programs generally written in Common Lisp
often favor recursion over iteration, or have inner loops so
complex that they involve multiple function calls. Such programs
spend a larger fraction of their time doing function calls than is
the norm in other languages; for this reason Common Lisp
implementations strive to make the general (or full) function call
as inexpensive as possible. Python goes beyond this by providing
two good alternatives to full call:
<ul>
<li>Local call resolves function references at compile time,
allowing better calling sequences and optimization across function
calls. See section&nbsp;<a href="#local-call">5.6</a>.<br>
<br></li>
<li>Inline expansion totally eliminates call overhead and allows
many context dependent optimizations. This provides a safe and
efficient implementation of operations with function semantics,
eliminating the need for error-prone macro definitions or manual
case analysis. Although most Common Lisp implementations support
inline expansion, it becomes a more powerful tool with Python's
source level optimization. See sections <a href=
"#source-optimization">5.4</a> and <a href=
"#inline-expansion">5.8</a>.</li>
</ul>
Generally, Python provides simple implementations for simple uses
of function call, rather than having only a single calling
convention. These features allow a more natural programming style:
<ul>
<li>Proper tail recursion. See section&nbsp;<a href=
"#tail-recursion">5.5</a><br>
<br></li>
<li>Relatively efficient closures.<br>
<br></li>
<li>A <tt class="code">funcall</tt> that is as efficient as normal
named call.<br>
<br></li>
<li>Calls to local functions such as from <tt class=
"code">labels</tt> are optimized:
<ul>
<li>Control transfer is a direct jump.<br>
<br></li>
<li>The closure environment is passed in registers rather than heap
allocated.<br>
<br></li>
<li>Keyword arguments and multiple values are implemented more
efficiently.</li>
</ul>
<br>
See section&nbsp;<a href="#local-call">5.6</a>.</li>
</ul>
<a name="toc137" id="toc137"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc130" id="htoc130"><b><font size=
"4">5.1.4</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Representation of
Objects</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<br>
Sometimes traditional Common Lisp implementation techniques compare
so poorly to the techniques used in other languages that Common
Lisp can become an impractical language choice. Terrible
inefficiencies appear in number-crunching programs, since Common
Lisp numeric operations often involve number-consing and generic
arithmetic. Python supports efficient natural representations for
numbers (and some other types), and allows these efficient
representations to be used in more contexts. Python also provides
good efficiency notes that warn when a crucial declaration is
missing.<br>
<br>
See section <a href="#non-descriptor">5.11.2</a> for more about
object representations and numeric types. Also see
section&nbsp;<a href="#efficiency-notes">5.13</a> about efficiency
notes.<br>
<br>
<a name="toc138" id="toc138"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc131" id="htoc131"><b><font size=
"4">5.1.5</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Writing Efficient
Code</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="efficiency-overview" id="efficiency-overview"></a><br>
Writing efficient code that works is a complex and prolonged
process. It is important not to get so involved in the pursuit of
efficiency that you lose sight of what the original problem
demands. Remember that:
<ul>
<li>The program should be correct---it doesn't matter how quickly
you get the wrong answer.<br>
<br></li>
<li>Both the programmer and the user will make errors, so the
program must be robust---it must detect errors in a way that allows
easy correction.<br>
<br></li>
<li>A small portion of the program will consume most of the
resources, with the bulk of the code being virtually irrelevant to
efficiency considerations. Even experienced programmers familiar
with the problem area cannot reliably predict where these ``hot
spots'' will be.</li>
</ul>
The best way to get efficient code that is still worth using, is to
separate coding from tuning. During coding, you should:
<ul>
<li>Use a coding style that aids correctness and robustness without
being incompatible with efficiency.<br>
<br></li>
<li>Choose appropriate data structures that allow efficient
algorithms and object representations (see section&nbsp;<a href=
"#object-representation">5.10</a>). Try to make interfaces abstract
enough so that you can change to a different representation if
profiling reveals a need.<br>
<br></li>
<li>Whenever you make an assumption about a function argument or
global data structure, add consistency assertions, either with type
declarations or explicit uses of <tt class="code">assert</tt>,
<tt class="code">ecase</tt>, etc.</li>
</ul>
During tuning, you should:
<ul>
<li>Identify the hot spots in the program through profiling
(section <a href="#profiling">5.14</a>.)<br>
<br></li>
<li>Identify inefficient constructs in the hot spot with efficiency
notes, more profiling, or manual inspection of the source. See
sections <a href="#general-efficiency">5.12</a> and <a href=
"#efficiency-notes">5.13</a>.<br>
<br></li>
<li>Add declarations and consider the application of optimizations.
See sections <a href="#local-call">5.6</a>, <a href=
"#inline-expansion">5.8</a> and <a href=
"#non-descriptor">5.11.2</a>.<br>
<br></li>
<li>If all else fails, consider algorithm or data structure
changes. If you did a good job coding, changes will be easy to
introduce.</li>
</ul>
<a name="toc139" id="toc139"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFD3">
<div align="center">
<table>
<tr>
<td><a name="htoc132" id="htoc132"><b><font size=
"5">5.2</font></b></a></td>
<td width="100%" align="center"><b><font size="5">More About Types
in Python</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="advanced-type-stuff" id="advanced-type-stuff"></a>
<a name="@concept134"></a><br>
This section goes into more detail describing what types and
declarations are recognized by Python. The area where Python
differs most radically from previous Common Lisp compilers is in
its support for types:
<ul>
<li>Precise type checking helps to find bugs at run time.<br>
<br></li>
<li>Compile-time type checking helps to find bugs at compile
time.<br>
<br></li>
<li>Type inference minimizes the need for generic operations, and
also increases the efficiency of run time type checking and the
effectiveness of compile time type checking.<br>
<br></li>
<li>Support for detailed types provides a wealth of opportunity for
operation-specific type inference and optimization.</li>
</ul>
<a name="toc140" id="toc140"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc133" id="htoc133"><b><font size=
"4">5.2.1</font></b></a></td>
<td width="100%" align="center"><b><font size="4">More Types
Meaningful</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<br>
Common Lisp has a very powerful type system, but conventional
Common Lisp implementations typically only recognize the small set
of types special in that implementation. In these systems, there is
an unfortunate paradox: a declaration for a relatively general type
like <tt class="code">fixnum</tt> will be recognized by the
compiler, but a highly specific declaration such as <tt class=
"code">(integer 3 17)</tt> is totally ignored.<br>
<br>
This is obviously a problem, since the user has to know how to
specify the type of an object in the way the compiler wants it. A
very minimal (but rarely satisfied) criterion for type system
support is that it be no worse to make a specific declaration than
to make a general one. Python goes beyond this by exploiting a
number of advantages obtained from detailed type information.<br>
<br>
Using more restrictive types in declarations allows the compiler to
do better type inference and more compile-time type checking. Also,
when type declarations are considered to be consistency assertions
that should be verified (conditional on policy), then complex types
are useful for making more detailed assertions.<br>
<br>
Python ``understands'' the list-style <tt class="code">or</tt>,
<tt class="code">member</tt>, <tt class="code">function</tt>, array
and number type specifiers. Understanding means that:
<ul>
<li>If the type contains more information than is used in a
particular context, then the extra information is simply ignored,
rather than derailing type inference.<br>
<br></li>
<li>In many contexts, the extra information from these type
specifier is used to good effect. In particular, type checking in
Python is <tt class="variable">precise</tt>, so these complex types
can be used in declarations to make interesting assertions about
functions and data structures (see section&nbsp;<a href=
"compiler.html#precise-type-checks">4.5.2</a>.) More specific
declarations also aid type inference and reduce the cost for type
checking.</li>
</ul>
For related information, see section&nbsp;<a href=
"#numeric-types">5.11</a> for numeric types, and section <a href=
"#array-types">5.10.3</a> for array types.<br>
<br>
<a name="toc141" id="toc141"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc134" id="htoc134"><b><font size=
"4">5.2.2</font></b></a></td>
<td width="100%" align="center"><b><font size=
"4">Canonicalization</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept135"></a> <a name="@concept136"></a> <a name=
"@concept137"></a><br>
When given a type specifier, Python will often rewrite it into a
different (but equivalent) type. This is the mechanism that Python
uses for detecting type equivalence. For example, in Python's
canonical representation, these types are equivalent:
<blockquote class="example">
<pre>
(or list (member :end)) &lt;==&gt; (or cons (member nil :end))
</pre></blockquote>
This has two implications for the user:
<ul>
<li>The standard symbol type specifiers for <tt class=
"code">atom</tt>, <tt class="code">null</tt>, <tt class=
"code">fixnum</tt>, etc., are in no way magical. The <a name=
"@types22"></a><tt class="code">null</tt> type is actually defined
to be <tt class="code">(member nil)</tt>, <a name=
"@types23"></a><tt class="code">list</tt> is <tt class="code">(or
cons null)</tt>, and <a name="@types24"></a><tt class=
"code">fixnum</tt> is <tt class="code">(signed-byte 30)</tt>.<br>
<br></li>
<li>When the compiler prints out a type, it may not look like the
type specifier that originally appeared in the program. This is
generally not a problem, but it must be taken into consideration
when reading compiler error messages.</li>
</ul>
<a name="toc142" id="toc142"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc135" id="htoc135"><b><font size=
"4">5.2.3</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Member
Types</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept138"></a><br>
The <a name="@types25"></a><tt class="code">member</tt> type
specifier can be used to represent ``symbolic'' values, analogous
to the enumerated types of Pascal. For example, the second value of
<tt class="code">find-symbol</tt> has this type:
<blockquote class="lisp">
<pre>
(member :internal :external :inherited nil)
</pre></blockquote>
Member types are very useful for expressing consistency constraints
on data structures, for example:
<blockquote class="lisp">
<pre>
(defstruct ice-cream
  (flavor :vanilla :type (member :vanilla :chocolate :strawberry)))
</pre></blockquote>
Member types are also useful in type inference, as the number of
members can sometimes be pared down to one, in which case the value
is a known constant.<br>
<br>
<a name="toc143" id="toc143"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc136" id="htoc136"><b><font size=
"4">5.2.4</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Union
Types</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept139"></a> <a name="@concept140"></a><br>
The <a name="@types26"></a><tt class="code">or</tt> (union) type
specifier is understood, and is meaningfully applied in many
contexts. The use of <tt class="code">or</tt> allows assertions to
be made about types in dynamically typed programs. For example:
<blockquote class="lisp">
<pre>
(defstruct box
  (next nil :type (or box null))
  (top :removed :type (or box-top (member :removed))))
</pre></blockquote>
The type assertion on the <tt class="code">top</tt> slot ensures
that an error will be signaled when there is an attempt to store an
illegal value (such as <tt class="code">:rmoved</tt>.) Although
somewhat weak, these union type assertions provide a useful input
into type inference, allowing the cost of type checking to be
reduced. For example, this loop is safely compiled with no type
checks:
<blockquote class="lisp">
<pre>
(defun find-box-with-top (box)
  (declare (type (or box null) box))
  (do ((current box (box-next current)))
      ((null current))
    (unless (eq (box-top current) :removed)
      (return current))))
</pre></blockquote>
Union types are also useful in type inference for representing
types that are partially constrained. For example, the result of
this expression:
<blockquote class="lisp">
<pre>
(if foo
    (logior x y)
    (list x y))
</pre></blockquote>
can be expressed as <tt class="code">(or integer cons)</tt>.<br>
<br>
<a name="toc144" id="toc144"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc137" id="htoc137"><b><font size=
"4">5.2.5</font></b></a></td>
<td width="100%" align="center"><b><font size="4">The Empty
Type</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="empty-type" id="empty-type"></a> <a name=
"@concept141"></a> <a name="@concept142"></a> <a name=
"@concept143"></a><br>
The type <tt class="code">nil</tt> is also called the empty type,
since no object is of type <tt class="code">nil</tt>. The union of
no types, <tt class="code">(or)</tt>, is also empty. Python's
interpretation of an expression whose type is <tt class=
"code">nil</tt> is that the expression never yields any value, but
rather fails to terminate, or is thrown out of. For example, the
type of a call to <tt class="code">error</tt> or a use of
<tt class="code">return</tt> is <tt class="code">nil</tt>. When the
type of an expression is empty, compile-time type warnings about
its value are suppressed; presumably somebody else is signaling an
error. If a function is declared to have return type <tt class=
"code">nil</tt>, but does in fact return, then (in safe compilation
policies) a ``<tt class="code">NIL Function returned</tt>'' error
will be signaled. See also the function <a name=
"@funs118"></a><tt class="code">required-argument</tt>.<br>
<br>
<a name="toc145" id="toc145"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc138" id="htoc138"><b><font size=
"4">5.2.6</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Function
Types</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="function-types" id="function-types"></a> <a name=
"@concept144"></a> <a name="@concept145"></a><br>
<a name="@funs119"></a><tt class="code">function</tt> types are
understood in the restrictive sense, specifying:
<ul>
<li>The argument syntax that the function must be called with. This
is information about what argument counts are acceptable, and which
keyword arguments are recognized. In Python, warnings about
argument syntax are a consequence of function type checking.<br>
<br></li>
<li>The types of the argument values that the caller must pass. If
the compiler can prove that some argument to a call is of a type
disallowed by the called function's type, then it will give a
compile-time type warning. In addition to being used for
compile-time type checking, these type assertions are also used as
output type assertions in code generation. For example, if
<tt class="code">foo</tt> is declared to have a <tt class=
"code">fixnum</tt> argument, then the <tt class="code">1+</tt> in
<tt class="code">(foo (1+ x))</tt> is compiled with knowledge that
the result must be a fixnum.<br>
<br></li>
<li>The types the values that will be bound to argument variables
in the function's definition. Declaring a function's type with
<tt class="code">ftype</tt> implicitly declares the types of the
arguments in the definition. Python checks for consistency between
the definition and the <tt class="code">ftype</tt> declaration.
Because of precise type checking, an error will be signaled when a
function is called with an argument of the wrong type.<br>
<br></li>
<li>The type of return value(s) that the caller can expect. This
information is a useful input to type inference. For example, if a
function is declared to return a <tt class="code">fixnum</tt>, then
when a call to that function appears in an expression, the
expression will be compiled with knowledge that the call will
return a <tt class="code">fixnum</tt>.<br>
<br></li>
<li>The type of return value(s) that the definition must return.
The result type in an <tt class="code">ftype</tt> declaration is
treated like an implicit <tt class="code">the</tt> wrapped around
the body of the definition. If the definition returns a value of
the wrong type, an error will be signaled. If the compiler can
prove that the function returns the wrong type, then it will give a
compile-time warning.</li>
</ul>
This is consistent with the new interpretation of function types
and the <tt class="code">ftype</tt> declaration in the proposed
X3J13 ``function-type-argument-type-semantics'' cleanup. Note also,
that if you don't explicitly declare the type of a function using a
global <tt class="code">ftype</tt> declaration, then Python will
compute a function type from the definition, providing a degree of
inter-routine type inference, see section&nbsp;<a href=
"#function-type-inference">5.3.3</a>.<br>
<br>
<a name="toc146" id="toc146"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc139" id="htoc139"><b><font size=
"4">5.2.7</font></b></a></td>
<td width="100%" align="center"><b><font size="4">The Values
Declaration</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept146"></a><br>
CMUCL supports the <tt class="code">values</tt> declaration as an
extension to Common Lisp. The syntax of the declaration is
<tt class="code">(values <tt class="variable">type1</tt> <tt class=
"variable">type2</tt>...<tt class="variable">typen</tt>)</tt>. This
declaration is semantically equivalent to a <tt class=
"code">the</tt> form wrapped around the body of the special form in
which the <tt class="code">values</tt> declaration appears. The
advantage of <tt class="code">values</tt> over <a name=
"@funs120"></a><tt class="code">the</tt> is purely syntactic---it
doesn't introduce more indentation. For example:
<blockquote class="example">
<pre>
(defun foo (x)
  (declare (values single-float))
  (ecase x
    (:this ...)
    (:that ...)
    (:the-other ...)))
</pre></blockquote>
is equivalent to:
<blockquote class="example">
<pre>
(defun foo (x)
  (the single-float
       (ecase x
         (:this ...)
         (:that ...)
         (:the-other ...))))
</pre></blockquote>
and
<blockquote class="example">
<pre>
(defun floor (number &amp;optional (divisor 1))
  (declare (values integer real))
  ...)
</pre></blockquote>
is equivalent to:
<blockquote class="example">
<pre>
(defun floor (number &amp;optional (divisor 1))
  (the (values integer real)
       ...))
</pre></blockquote>
In addition to being recognized by <tt class="code">lambda</tt>
(and hence by <tt class="code">defun</tt>), the <tt class=
"code">values</tt> declaration is recognized by all the other
special forms with bodies and declarations: <tt class=
"code">let</tt>, <tt class="code">let*</tt>, <tt class=
"code">labels</tt> and <tt class="code">flet</tt>. Macros with
declarations usually splice the declarations into one of the above
forms, so they will accept this declaration too, but the exact
effect of a <tt class="code">values</tt> declaration will depend on
the macro.<br>
<br>
If you declare the types of all arguments to a function, and also
declare the return value types with <tt class="code">values</tt>,
you have described the type of the function. Python will use this
argument and result type information to derive a function type that
will then be applied to calls of the function (see
section&nbsp;<a href="#function-types">5.2.6</a>.) This provides a
way to declare the types of functions that is much less
syntactically awkward than using the <tt class="code">ftype</tt>
declaration with a <tt class="code">function</tt> type
specifier.<br>
<br>
Although the <tt class="code">values</tt> declaration is
non-standard, it is relatively harmless to use it in otherwise
portable code, since any warning in non-CMU implementations can be
suppressed with the standard <tt class="code">declaration</tt>
proclamation.<br>
<br>
<a name="toc147" id="toc147"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc140" id="htoc140"><b><font size=
"4">5.2.8</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Structure
Types</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="structure-types" id="structure-types"></a> <a name=
"@concept147"></a> <a name="@concept148"></a> <a name=
"@concept149"></a><br>
Because of precise type checking, structure types are much better
supported by Python than by conventional compilers:
<ul>
<li>The structure argument to structure accessors is precisely
checked---if you call <tt class="code">foo-a</tt> on a <tt class=
"code">bar</tt>, an error will be signaled.<br>
<br></li>
<li>The types of slot values are precisely checked---if you pass
the wrong type argument to a constructor or a slot setter, then an
error will be signaled.</li>
</ul>
This error checking is tremendously useful for detecting bugs in
programs that manipulate complex data structures.<br>
<br>
An additional advantage of checking structure types and enforcing
slot types is that the compiler can safely believe slot type
declarations. Python effectively moves the type checking from the
slot access to the slot setter or constructor call. This is more
efficient since caller of the setter or constructor often knows the
type of the value, entirely eliminating the need to check the
value's type. Consider this example:
<blockquote class="lisp">
<pre>
(defstruct coordinate
  (x nil :type single-float)
  (y nil :type single-float))

(defun make-it ()
  (make-coordinate :x 1.0 :y 1.0))

(defun use-it (it)
  (declare (type coordinate it))
  (sqrt (expt (coordinate-x it) 2) (expt (coordinate-y it) 2)))
</pre></blockquote>
<br>
<br>
<tt class="code">make-it</tt> and <tt class="code">use-it</tt> are
compiled with no checking on the types of the float slots, yet
<tt class="code">use-it</tt> can use <tt class=
"code">single-float</tt> arithmetic with perfect safety. Note that
<tt class="code">make-coordinate</tt> must still check the values
of <tt class="code">x</tt> and <tt class="code">y</tt> unless the
call is block compiled or inline expanded (see
section&nbsp;<a href="#local-call">5.6</a>.) But even without this
advantage, it is almost always more efficient to check slot values
on structure initialization, since slots are usually written once
and read many times.<br>
<br>
<a name="toc148" id="toc148"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc141" id="htoc141"><b><font size=
"4">5.2.9</font></b></a></td>
<td width="100%" align="center"><b><font size="4">The Freeze-Type
Declaration</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept150"></a> <a name="freeze-type" id=
"freeze-type"></a><br>
The <tt class="code">extensions:freeze-type</tt> declaration is a
CMUCL extension that enables more efficient compilation of
user-defined types by asserting that the definition is not going to
change. This declaration may only be used globally (with <tt class=
"code">declaim</tt> or <tt class="code">proclaim</tt>). Currently
<tt class="code">freeze-type</tt> only affects structure type
testing done by <tt class="code">typep</tt>, <tt class=
"code">typecase</tt>, etc. Here is an example:
<blockquote class="lisp">
<pre>
(declaim (freeze-type foo bar))
</pre></blockquote>
This asserts that the types <tt class="code">foo</tt> and
<tt class="code">bar</tt> and their subtypes are not going to
change. This allows more efficient type testing, since the compiler
can open-code a test for all possible subtypes, rather than having
to examine the type hierarchy at run-time.<br>
<br>
<a name="toc149" id="toc149"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc142" id="htoc142"><b><font size=
"4">5.2.10</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Type
Restrictions</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept151"></a><br>
Avoid use of the <tt class="code">and</tt>, <tt class=
"code">not</tt> and <tt class="code">satisfies</tt> types in
declarations, since type inference has problems with them. When
these types do appear in a declaration, they are still checked
precisely, but the type information is of limited use to the
compiler. <tt class="code">and</tt> types are effective as long as
the intersection can be canonicalized to a type that doesn't use
<tt class="code">and</tt>. For example:
<blockquote class="example">
<pre>
(and fixnum unsigned-byte)
</pre></blockquote>
is fine, since it is the same as:
<blockquote class="example">
<pre>
(integer 0 <tt class="variable">most-positive-fixnum</tt>)
</pre></blockquote>
but this type:
<blockquote class="example">
<pre>
(and symbol (not (member :end)))
</pre></blockquote>
will not be fully understood by type interference since the
<tt class="code">and</tt> can't be removed by canonicalization.<br>
<br>
Using any of these type specifiers in a type test with <tt class=
"code">typep</tt> or <tt class="code">typecase</tt> is fine, since
as tests, these types can be translated into the <tt class=
"code">and</tt> macro, the <tt class="code">not</tt> function or a
call to the satisfies predicate.<br>
<br>
<a name="toc150" id="toc150"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc143" id="htoc143"><b><font size=
"4">5.2.11</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Type Style
Recommendations</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept152"></a><br>
Python provides good support for some currently unconventional ways
of using the Common Lisp type system. With Python, it is desirable
to make declarations as precise as possible, but type inference
also makes some declarations unnecessary. Here are some general
guidelines for maximum robustness and efficiency:
<ul>
<li>Declare the types of all function arguments and structure slots
as precisely as possible (while avoiding <tt class="code">not</tt>,
<tt class="code">and</tt> and <tt class="code">satisfies</tt>). Put
these declarations in during initial coding so that type assertions
can find bugs for you during debugging.<br>
<br></li>
<li>Use the <a name="@types27"></a><tt class="code">member</tt>
type specifier where there are a small number of possible symbol
values, for example: <tt class="code">(member :red :blue
:green)</tt>.<br>
<br></li>
<li>Use the <a name="@types28"></a><tt class="code">or</tt> type
specifier in situations where the type is not certain, but there
are only a few possibilities, for example: <tt class="code">(or
list vector)</tt>.<br>
<br></li>
<li>Declare integer types with the tightest bounds that you can,
such as <tt class="code">(integer 3 7)</tt>.<br>
<br></li>
<li>Define <a name="@funs121"></a><tt class="code">deftype</tt> or
<a name="@funs122"></a><tt class="code">defstruct</tt> types before
they are used. Definition after use is legal (producing no
``undefined type'' warnings), but type tests and structure
operations will be compiled much less efficiently.<br>
<br></li>
<li>Use the <tt class="code">extensions:freeze-type</tt>
declaration to speed up type testing for structure types which
won't have new subtypes added later. See section&nbsp;<a href=
"#freeze-type">5.2.9</a><br>
<br></li>
<li>In addition to declaring the array element type and simpleness,
also declare the dimensions if they are fixed, for example:
<blockquote class="example">
<pre>
    (simple-array single-float (1024 1024))
  
</pre></blockquote>
This bounds information allows array indexing for multi-dimensional
arrays to be compiled much more efficiently, and may also allow
array bounds checking to be done at compile time. See
section&nbsp;<a href="#array-types">5.10.3</a>.<br>
<br></li>
<li>Avoid use of the <a name="@funs123"></a><tt class=
"code">the</tt> declaration within expressions. Not only does it
clutter the code, but it is also almost worthless under safe
policies. If the need for an output type assertion is revealed by
efficiency notes during tuning, then you can consider <tt class=
"code">the</tt>, but it is preferable to constrain the argument
types more, allowing the compiler to prove the desired result
type.<br>
<br></li>
<li>Don't bother declaring the type of <a name=
"@funs124"></a><tt class="code">let</tt> or other non-argument
variables unless the type is non-obvious. If you declare function
return types and structure slot types, then the type of a variable
is often obvious both to the programmer and to the compiler. An
important case where the type isn't obvious, and a declaration is
appropriate, is when the value for a variable is pulled out of
untyped structure (e.g., the result of <tt class="code">car</tt>),
or comes from some weakly typed function, such as <tt class=
"code">read</tt>.<br>
<br></li>
<li>Declarations are sometimes necessary for integer loop
variables, since the compiler can't always prove that the value is
of a good integer type. These declarations are best added during
tuning, when an efficiency note indicates the need.</li>
</ul>
<a name="toc151" id="toc151"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFD3">
<div align="center">
<table>
<tr>
<td><a name="htoc144" id="htoc144"><b><font size=
"5">5.3</font></b></a></td>
<td width="100%" align="center"><b><font size="5">Type
Inference</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="type-inference" id="type-inference"></a> <a name=
"@concept153"></a> <a name="@concept154"></a> <a name=
"@concept155"></a><br>
Type inference is the process by which the compiler tries to figure
out the types of expressions and variables, given an inevitable
lack of complete type information. Although Python does much more
type inference than most Common Lisp compilers, remember that the
more precise and comprehensive type declarations are, the more type
inference will be able to do.<br>
<br>
<a name="toc152" id="toc152"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc145" id="htoc145"><b><font size=
"4">5.3.1</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Variable Type
Inference</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="variable-type-inference" id=
"variable-type-inference"></a><br>
The type of a variable is the union of the types of all the
definitions. In the degenerate case of a let, the type of the
variable is the type of the initial value. This inferred type is
intersected with any declared type, and is then propagated to all
the variable's references. The types of <a name=
"@funs125"></a><tt class="code">multiple-value-bind</tt> variables
are similarly inferred from the types of the individual values of
the values form.<br>
<br>
If multiple type declarations apply to a single variable, then all
the declarations must be correct; it is as though all the types
were intersected producing a single <a name=
"@types29"></a><tt class="code">and</tt> type specifier. In this
example:
<blockquote class="example">
<pre>
(defmacro my-dotimes ((var count) &amp;body body)
  `(do ((,var 0 (1+ ,var)))
       ((&gt;= ,var ,count))
     (declare (type (integer 0 *) ,var))
     ,@body))

(my-dotimes (i ...)
  (declare (fixnum i))
  ...)
</pre></blockquote>
the two declarations for <tt class="code">i</tt> are intersected,
so <tt class="code">i</tt> is known to be a non-negative
fixnum.<br>
<br>
In practice, this type inference is limited to lets and local
functions, since the compiler can't analyze all the calls to a
global function. But type inference works well enough on local
variables so that it is often unnecessary to declare the type of
local variables. This is especially likely when function result
types and structure slot types are declared. The main areas where
type inference breaks down are:
<ul>
<li>When the initial value of a variable is a untyped expression,
such as <tt class="code">(car x)</tt>, and<br>
<br></li>
<li>When the type of one of the variable's definitions is a
function of the variable's current value, as in: <tt class=
"code">(setq x (1+ x))</tt></li>
</ul>
<a name="toc153" id="toc153"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc146" id="htoc146"><b><font size=
"4">5.3.2</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Local Function
Type Inference</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept156"></a><br>
The types of arguments to local functions are inferred in the same
was as any other local variable; the type is the union of the
argument types across all the calls to the function, intersected
with the declared type. If there are any assignments to the
argument variables, the type of the assigned value is unioned in as
well.<br>
<br>
The result type of a local function is computed in a special way
that takes tail recursion (see section&nbsp;<a href=
"#tail-recursion">5.5</a>) into consideration. The result type is
the union of all possible return values that aren't tail-recursive
calls. For example, Python will infer that the result type of this
function is <tt class="code">integer</tt>:
<blockquote class="lisp">
<pre>
(defun ! (n res)
  (declare (integer n res))
  (if (zerop n)
      res
      (! (1- n) (* n res))))
</pre></blockquote>
Although this is a rather obvious result, it becomes somewhat less
trivial in the presence of mutual tail recursion of multiple
functions. Local function result type inference interacts with the
mechanisms for ensuring proper tail recursion mentioned in section
<a href="#local-call-return">5.6.5</a>.<br>
<br>
<a name="toc154" id="toc154"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc147" id="htoc147"><b><font size=
"4">5.3.3</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Global Function
Type Inference</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="function-type-inference" id="function-type-inference"></a>
<a name="@concept157"></a><br>
As described in section <a href="#function-types">5.2.6</a>, a
global function type (<a name="@types30"></a><tt class=
"code">ftype</tt>) declaration places implicit type assertions on
the call arguments, and also guarantees the type of the return
value. So wherever a call to a declared function appears, there is
no doubt as to the types of the arguments and return value.
Furthermore, Python will infer a function type from the function's
definition if there is no <tt class="code">ftype</tt> declaration.
Any type declarations on the argument variables are used as the
argument types in the derived function type, and the compiler's
best guess for the result type of the function is used as the
result type in the derived function type.<br>
<br>
This method of deriving function types from the definition
implicitly assumes that functions won't be redefined at run-time.
Consider this example:
<blockquote class="lisp">
<pre>
(defun foo-p (x)
  (let ((res (and (consp x) (eq (car x) 'foo))))
    (format t "It is ~:[not ~;~]foo." res)))

(defun frob (it)
  (if (foo-p it)
      (setf (cadr it) 'yow!)
      (1+ it)))
</pre></blockquote>
Presumably, the programmer really meant to return <tt class=
"code">res</tt> from <tt class="code">foo-p</tt>, but he seems to
have forgotten. When he tries to call do <tt class="code">(frob
(list 'foo nil))</tt>, <tt class="code">frob</tt> will flame out
when it tries to add to a <tt class="code">cons</tt>. Realizing his
error, he fixes <tt class="code">foo-p</tt> and recompiles it. But
when he retries his test case, he is baffled because the error is
still there. What happened in this example is that Python proved
that the result of <tt class="code">foo-p</tt> is <tt class=
"code">null</tt>, and then proceeded to optimize away the
<tt class="code">setf</tt> in <tt class="code">frob</tt>.<br>
<br>
Fortunately, in this example, the error is detected at compile time
due to notes about unreachable code (see section&nbsp;<a href=
"#dead-code-notes">5.4.5</a>.) Still, some users may not want to
worry about this sort of problem during incremental development, so
there is a variable to control deriving function types.<br>
<br>
<br>
<a name="@vars47"></a><a name="VR:derive-function-types" id=
"VR:derive-function-types"></a>
<div align="left">[Variable]<br>
<tt class="function-name">extensions:</tt><tt class=
"function-name">*derive-function-types*</tt>
&nbsp;&nbsp;&nbsp;</div>
<blockquote><br>
If true (the default), argument and result type information derived
from compilation of <tt class="code">defun</tt>s is used when
compiling calls to that function. If false, only information from
<tt class="code">ftype</tt> proclamations will be
used.</blockquote>
<a name="toc155" id="toc155"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc148" id="htoc148"><b><font size=
"4">5.3.4</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Operation
Specific Type Inference</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="operation-type-inference" id=
"operation-type-inference"></a> <a name="@concept158"></a> <a name=
"@concept159"></a> <a name="@concept160"></a><br>
Many of the standard Common Lisp functions have special type
inference procedures that determine the result type as a function
of the argument types. For example, the result type of <tt class=
"code">aref</tt> is the array element type. Here are some other
examples of type inferences:
<blockquote class="lisp">
<pre>
(logand x #xFF) ==&gt; (unsigned-byte 8)

(+ (the (integer 0 12) x) (the (integer 0 1) y)) ==&gt; (integer 0 13)

(ash (the (unsigned-byte 16) x) -8) ==&gt; (unsigned-byte 8)
</pre></blockquote>
<a name="toc156" id="toc156"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc149" id="htoc149"><b><font size=
"4">5.3.5</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Dynamic Type
Inference</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="constraint-propagation" id="constraint-propagation"></a>
<a name="@concept161"></a> <a name="@concept162"></a> <a name=
"@concept163"></a><br>
Python uses flow analysis to infer types in dynamically typed
programs. For example:
<blockquote class="example">
<pre>
(ecase x
  (list (length x))
  ...)
</pre></blockquote>
Here, the compiler knows the argument to <tt class=
"code">length</tt> is a list, because the call to <tt class=
"code">length</tt> is only done when <tt class="code">x</tt> is a
list. The most significant efficiency effect of inference from
assertions is usually in type check optimization.<br>
<br>
Dynamic type inference has two inputs: explicit conditionals and
implicit or explicit type assertions. Flow analysis propagates
these constraints on variable type to any code that can be executed
only after passing though the constraint. Explicit type constraints
come from <a name="@funs126"></a><tt class="code">if</tt>s where
the test is either a lexical variable or a function of lexical
variables and constants, where the function is either a type
predicate, a numeric comparison or <tt class="code">eq</tt>.<br>
<br>
If there is an <tt class="code">eq</tt> (or <tt class=
"code">eql</tt>) test, then the compiler will actually substitute
one argument for the other in the true branch. For example:
<blockquote class="lisp">
<pre>
(when (eq x :yow!) (return x))
</pre></blockquote>
becomes:
<blockquote class="lisp">
<pre>
(when (eq x :yow!) (return :yow!))
</pre></blockquote>
This substitution is done when one argument is a constant, or one
argument has better type information than the other. This
transformation reveals opportunities for constant folding or
type-specific optimizations. If the test is against a constant,
then the compiler can prove that the variable is not that constant
value in the false branch, or <tt class="code">(not (member
:yow!))</tt> in the example above. This can eliminate redundant
tests, for example:
<blockquote class="example">
<pre>
(if (eq x nil)
    ...
    (if x a b))
</pre></blockquote>
is transformed to this:
<blockquote class="example">
<pre>
(if (eq x nil)
    ...
    a)
</pre></blockquote>
Variables appearing as <tt class="code">if</tt> tests are
interpreted as <tt class="code">(not (eq <tt class=
"variable">var</tt> nil))</tt> tests. The compiler also converts
<tt class="code">=</tt> into <tt class="code">eql</tt> where
possible. It is difficult to do inference directly on <tt class=
"code">=</tt> since it does implicit coercions.<br>
<br>
When there is an explicit or test on numeric variables, the
compiler makes inferences about the ranges the variables can assume
in the true and false branches. This is mainly useful when it
proves that the values are small enough in magnitude to allow
open-coding of arithmetic operations. For example, in many uses of
<tt class="code">dotimes</tt> with a <tt class="code">fixnum</tt>
repeat count, the compiler proves that fixnum arithmetic can be
used.<br>
<br>
Implicit type assertions are quite common, especially if you
declare function argument types. Dynamic inference from implicit
type assertions sometimes helps to disambiguate programs to a
useful degree, but is most noticeable when it detects a dynamic
type error. For example:
<blockquote class="lisp">
<pre>
(defun foo (x)
  (+ (car x) x))
</pre></blockquote>
results in this warning:
<blockquote class="example">
<pre>
In: DEFUN FOO
  (+ (CAR X) X)
==&gt;
  X
Warning: Result is a LIST, not a NUMBER.
</pre></blockquote>
Note that Common Lisp's dynamic type checking semantics make
dynamic type inference useful even in programs that aren't really
dynamically typed, for example:
<blockquote class="lisp">
<pre>
(+ (car x) (length x))
</pre></blockquote>
Here, <tt class="code">x</tt> presumably always holds a list, but
in the absence of a declaration the compiler cannot assume
<tt class="code">x</tt> is a list simply because list-specific
operations are sometimes done on it. The compiler must consider the
program to be dynamically typed until it proves otherwise. Dynamic
type inference proves that the argument to <tt class=
"code">length</tt> is always a list because the call to <tt class=
"code">length</tt> is only done after the list-specific <tt class=
"code">car</tt> operation.<br>
<br>
<a name="toc157" id="toc157"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc150" id="htoc150"><b><font size=
"4">5.3.6</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Type Check
Optimization</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="type-check-optimization" id="type-check-optimization"></a>
<a name="@concept164"></a> <a name="@concept165"></a><br>
Python backs up its support for precise type checking by minimizing
the cost of run-time type checking. This is done both through type
inference and though optimizations of type checking itself.<br>
<br>
Type inference often allows the compiler to prove that a value is
of the correct type, and thus no type check is necessary. For
example:
<blockquote class="lisp">
<pre>
(defstruct foo a b c)
(defstruct link
  (foo (required-argument) :type foo)
  (next nil :type (or link null)))

(foo-a (link-foo x))
</pre></blockquote>
Here, there is no need to check that the result of <tt class=
"code">link-foo</tt> is a <tt class="code">foo</tt>, since it
always is. Even when some type checks are necessary, type inference
can often reduce the number:
<blockquote class="example">
<pre>
(defun test (x)
  (let ((a (foo-a x))
        (b (foo-b x))
        (c (foo-c x)))
    ...))
</pre></blockquote>
In this example, only one <tt class="code">(foo-p x)</tt> check is
needed. This applies to a lesser degree in list operations, such
as:
<blockquote class="lisp">
<pre>
(if (eql (car x) 3) (cdr x) y)
</pre></blockquote>
Here, we only have to check that <tt class="code">x</tt> is a list
once.<br>
<br>
Since Python recognizes explicit type tests, code that explicitly
protects itself against type errors has little introduced overhead
due to implicit type checking. For example, this loop compiles with
no implicit checks checks for <tt class="code">car</tt> and
<tt class="code">cdr</tt>:
<blockquote class="lisp">
<pre>
(defun memq (e l)
  (do ((current l (cdr current)))
      ((atom current) nil)
    (when (eq (car current) e) (return current))))
</pre></blockquote>
<a name="@concept166"></a> Python reduces the cost of checks that
must be done through an optimization called <tt class=
"variable">complementing</tt>. A complemented check for <tt class=
"variable">type</tt> is simply a check that the value is not of the
type <tt class="code">(not <tt class="variable">type</tt>)</tt>.
This is only interesting when something is known about the actual
type, in which case we can test for the complement of <tt class=
"code">(and <tt class="variable">known-type</tt> (not <tt class=
"variable">type</tt>))</tt>, or the difference between the known
type and the assertion. An example:
<blockquote class="lisp">
<pre>
(link-foo (link-next x))
</pre></blockquote>
Here, we change the type check for <tt class="code">link-foo</tt>
from a test for <tt class="code">foo</tt> to a test for:
<blockquote class="lisp">
<pre>
(not (and (or foo null) (not foo)))
</pre></blockquote>
or more simply <tt class="code">(not null)</tt>. This is probably
the most important use of complementing, since the situation is
fairly common, and a <tt class="code">null</tt> test is much
cheaper than a structure type test.<br>
<br>
Here is a more complicated example that illustrates the combination
of complementing with dynamic type inference:
<blockquote class="lisp">
<pre>
(defun find-a (a x)
  (declare (type (or link null) x))
  (do ((current x (link-next current)))
      ((null current) nil)
    (let ((foo (link-foo current)))
      (when (eq (foo-a foo) a) (return foo)))))
</pre></blockquote>
This loop can be compiled with no type checks. The <tt class=
"code">link</tt> test for <tt class="code">link-foo</tt> and
<tt class="code">link-next</tt> is complemented to <tt class=
"code">(not null)</tt>, and then deleted because of the explicit
<tt class="code">null</tt> test. As before, no check is necessary
for <tt class="code">foo-a</tt>, since the <tt class=
"code">link-foo</tt> is always a <tt class="code">foo</tt>. This
sort of situation shows how precise type checking combined with
precise declarations can actually result in reduced type
checking.<br>
<br>
<a name="toc158" id="toc158"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFD3">
<div align="center">
<table>
<tr>
<td><a name="htoc151" id="htoc151"><b><font size=
"5">5.4</font></b></a></td>
<td width="100%" align="center"><b><font size="5">Source
Optimization</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="source-optimization" id="source-optimization"></a>
<a name="@concept167"></a><br>
This section describes source-level transformations that Python
does on programs in an attempt to make them more efficient.
Although source-level optimizations can make existing programs more
efficient, the biggest advantage of this sort of optimization is
that it makes it easier to write efficient programs. If a clean,
straightforward implementation is can be transformed into an
efficient one, then there is no need for tricky and dangerous hand
optimization.<br>
<br>
<a name="toc159" id="toc159"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc152" id="htoc152"><b><font size=
"4">5.4.1</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Let
Optimization</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="let-optimization" id="let-optimization"></a><br>
<a name="@concept168"></a> <a name="@concept169"></a><br>
The primary optimization of let variables is to delete them when
they are unnecessary. Whenever the value of a let variable is a
constant, a constant variable or a constant (local or
non-notinline) function, the variable is deleted, and references to
the variable are replaced with references to the constant
expression. This is useful primarily in the expansion of macros or
inline functions, where argument values are often constant in any
given call, but are in general non-constant expressions that must
be bound to preserve order of evaluation. Let variable optimization
eliminates the need for macros to carefully avoid spurious
bindings, and also makes inline functions just as efficient as
macros.<br>
<br>
A particularly interesting class of constant is a local function.
Substituting for lexical variables that are bound to a function can
substantially improve the efficiency of functional programming
styles, for example:
<blockquote class="lisp">
<pre>
(let ((a #'(lambda (x) (zow x))))
  (funcall a 3))
</pre></blockquote>
effectively transforms to:
<blockquote class="lisp">
<pre>
(zow 3)
</pre></blockquote>
This transformation is done even when the function is a closure, as
in:
<blockquote class="lisp">
<pre>
(let ((a (let ((y (zug)))
           #'(lambda (x) (zow x y)))))
  (funcall a 3))
</pre></blockquote>
becoming:
<blockquote class="lisp">
<pre>
(zow 3 (zug))
</pre></blockquote>
A constant variable is a lexical variable that is never assigned
to, always keeping its initial value. Whenever possible, avoid
setting lexical variables---instead bind a new variable to the new
value. Except for loop variables, it is almost always possible to
avoid setting lexical variables. This form:
<blockquote class="example">
<pre>
(let ((x (f x)))
  ...)
</pre></blockquote>
is <tt class="variable">more</tt> efficient than this form:
<blockquote class="example">
<pre>
(setq x (f x))
...
</pre></blockquote>
Setting variables makes the program more difficult to understand,
both to the compiler and to the programmer. Python compiles
assignments at least as efficiently as any other Common Lisp
compiler, but most let optimizations are only done on constant
variables.<br>
<br>
Constant variables with only a single use are also optimized away,
even when the initial value is not constant.<sup><a name="text11"
href="#note11" id="text11"><font size="2">1</font></a></sup> For
example, this expansion of <tt class="code">incf</tt>:
<blockquote class="lisp">
<pre>
(let ((#:g3 (+ x 1)))
  (setq x #:G3))
</pre></blockquote>
becomes:
<blockquote class="lisp">
<pre>
(setq x (+ x 1))
</pre></blockquote>
The type semantics of this transformation are more important than
the elimination of the variable itself. Consider what happens when
<tt class="code">x</tt> is declared to be a <tt class=
"code">fixnum</tt>; after the transformation, the compiler can
compile the addition knowing that the result is a <tt class=
"code">fixnum</tt>, whereas before the transformation the addition
would have to allow for fixnum overflow.<br>
<br>
Another variable optimization deletes any variable that is never
read. This causes the initial value and any assigned values to be
unused, allowing those expressions to be deleted if they have no
side-effects.<br>
<br>
Note that a let is actually a degenerate case of local call (see
section&nbsp;<a href="#let-calls">5.6.2</a>), and that let
optimization can be done on calls that weren't created by a let.
Also, local call allows an applicative style of iteration that is
totally assignment free.<br>
<br>
<a name="toc160" id="toc160"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc153" id="htoc153"><b><font size=
"4">5.4.2</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Constant
Folding</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept170"></a> <a name="@concept171"></a><br>
Constant folding is an optimization that replaces a call of
constant arguments with the constant result of that call. Constant
folding is done on all standard functions for which it is legal.
Inline expansion allows folding of any constant parts of the
definition, and can be done even on functions that have
side-effects.<br>
<br>
It is convenient to rely on constant folding when programming, as
in this example:
<blockquote class="example">
<pre>
(defconstant limit 42)

(defun foo ()
  (... (1- limit) ...))
</pre></blockquote>
Constant folding is also helpful when writing macros or inline
functions, since it usually eliminates the need to write a macro
that special-cases constant arguments.<br>
<br>
<a name="@concept172"></a> Constant folding of a user defined
function is enabled by the <tt class=
"code">extensions:constant-function</tt> proclamation. In this
example:
<blockquote class="example">
<pre>
(declaim (ext:constant-function myfun))
(defun myexp (x y)
  (declare (single-float x y))
  (exp (* (log x) y)))

 ... (myexp 3.0 1.3) ...
</pre></blockquote>
The call to <tt class="code">myexp</tt> is constant-folded to
<tt class="code">4.1711674</tt>.<br>
<br>
<a name="toc161" id="toc161"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc154" id="htoc154"><b><font size=
"4">5.4.3</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Unused Expression
Elimination</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept173"></a> <a name="@concept174"></a><br>
If the value of any expression is not used, and the expression has
no side-effects, then it is deleted. As with constant folding, this
optimization applies most often when cleaning up after inline
expansion and other optimizations. Any function declared an
<tt class="code">extensions:constant-function</tt> is also subject
to unused expression elimination.<br>
<br>
Note that Python will eliminate parts of unused expressions known
to be side-effect free, even if there are other unknown parts. For
example:
<blockquote class="lisp">
<pre>
(let ((a (list (foo) (bar))))
  (if t
      (zow)
      (raz a)))
</pre></blockquote>
becomes:
<blockquote class="lisp">
<pre>
(progn (foo) (bar))
(zow)
</pre></blockquote>
<a name="toc162" id="toc162"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc155" id="htoc155"><b><font size=
"4">5.4.4</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Control
Optimization</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept175"></a> <a name="@concept176"></a><br>
The most important optimization of control is recognizing when an
<a name="@funs127"></a><tt class="code">if</tt> test is known at
compile time, then deleting the <tt class="code">if</tt>, the test
expression, and the unreachable branch of the <tt class=
"code">if</tt>. This can be considered a special case of constant
folding, although the test doesn't have to be truly constant as
long as it is definitely not <tt class="code">nil</tt>. Note also,
that type inference propagates the result of an <tt class=
"code">if</tt> test to the true and false branches, see
section&nbsp;<a href="#constraint-propagation">5.3.5</a>.<br>
<br>
A related <tt class="code">if</tt> optimization is this
transformation:<sup><a name="text12" href="#note12" id=
"text12"><font size="2">2</font></a></sup>
<blockquote class="lisp">
<pre>
(if (if a b c) x y)
</pre></blockquote>
into:
<blockquote class="lisp">
<pre>
(if a
    (if b x y)
    (if c x y))
</pre></blockquote>
The opportunity for this sort of optimization usually results from
a conditional macro. For example:
<blockquote class="lisp">
<pre>
(if (not a) x y)
</pre></blockquote>
is actually implemented as this:
<blockquote class="lisp">
<pre>
(if (if a nil t) x y)
</pre></blockquote>
which is transformed to this:
<blockquote class="lisp">
<pre>
(if a
    (if nil x y)
    (if t x y))
</pre></blockquote>
which is then optimized to this:
<blockquote class="lisp">
<pre>
(if a y x)
</pre></blockquote>
Note that due to Python's internal representations, the <tt class=
"code">if</tt>---<tt class="code">if</tt> situation will be
recognized even if other forms are wrapped around the inner
<tt class="code">if</tt>, like:
<blockquote class="example">
<pre>
(if (let ((g ...))
      (loop
        ...
        (return (not g))
        ...))
    x y)
</pre></blockquote>
In Python, all the Common Lisp macros really are macros, written in
terms of <tt class="code">if</tt>, <tt class="code">block</tt> and
<tt class="code">tagbody</tt>, so user-defined control macros can
be just as efficient as the standard ones. Python emits basic
blocks using a heuristic that minimizes the number of unconditional
branches. The code in a <tt class="code">tagbody</tt> will not be
emitted in the order it appeared in the source, so there is no
point in arranging the code to make control drop through to the
target.<br>
<br>
<a name="toc163" id="toc163"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc156" id="htoc156"><b><font size=
"4">5.4.5</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Unreachable Code
Deletion</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="dead-code-notes" id="dead-code-notes"></a> <a name=
"@concept177"></a> <a name="@concept178"></a><br>
Python will delete code whenever it can prove that the code can
never be executed. Code becomes unreachable when:
<ul>
<li>An <tt class="code">if</tt> is optimized away, or<br>
<br></li>
<li>There is an explicit unconditional control transfer such as
<tt class="code">go</tt> or <tt class="code">return-from</tt>,
or<br>
<br></li>
<li>The last reference to a local function is deleted (or there
never was any reference.)</li>
</ul>
When code that appeared in the original source is deleted, the
compiler prints a note to indicate a possible problem (or at least
unnecessary code.) For example:
<blockquote class="lisp">
<pre>
(defun foo ()
  (if t
      (write-line "True.")
      (write-line "False.")))
</pre></blockquote>
will result in this note:
<blockquote class="example">
<pre>
In: DEFUN FOO
  (WRITE-LINE "False.")
Note: Deleting unreachable code.
</pre></blockquote>
It is important to pay attention to unreachable code notes, since
they often indicate a subtle type error. For example:
<blockquote class="example">
<pre>
(defstruct foo a b)

(defun lose (x)
  (let ((a (foo-a x))
        (b (if x (foo-b x) :none)))
    ...))
</pre></blockquote>
results in this note:
<blockquote class="example">
<pre>
In: DEFUN LOSE
  (IF X (FOO-B X) :NONE)
==&gt;
  :NONE
Note: Deleting unreachable code.
</pre></blockquote>
The <tt class="code">:none</tt> is unreachable, because type
inference knows that the argument to <tt class="code">foo-a</tt>
must be a <tt class="code">foo</tt>, and thus can't be <tt class=
"code">nil</tt>. Presumably the programmer forgot that <tt class=
"code">x</tt> could be <tt class="code">nil</tt> when he wrote the
binding for <tt class="code">a</tt>.<br>
<br>
Here is an example with an incorrect declaration:
<blockquote class="lisp">
<pre>
(defun count-a (string)
  (do ((pos 0 (position #\a string :start (1+ pos)))
       (count 0 (1+ count)))
      ((null pos) count)
    (declare (fixnum pos))))
</pre></blockquote>
This time our note is:
<blockquote class="example">
<pre>
In: DEFUN COUNT-A
  (DO ((POS 0 #) (COUNT 0 #))
      ((NULL POS) COUNT)
    (DECLARE (FIXNUM POS)))
--&gt; BLOCK LET TAGBODY RETURN-FROM PROGN 
==&gt;
  COUNT
Note: Deleting unreachable code.
</pre></blockquote>
The problem here is that <tt class="code">pos</tt> can never be
null since it is declared a <tt class="code">fixnum</tt>.<br>
<br>
It takes some experience with unreachable code notes to be able to
tell what they are trying to say. In non-obvious cases, the best
thing to do is to call the function in a way that should cause the
unreachable code to be executed. Either you will get a type error,
or you will find that there truly is no way for the code to be
executed.<br>
<br>
Not all unreachable code results in a note:
<ul>
<li>A note is only given when the unreachable code textually
appears in the original source. This prevents spurious notes due to
the optimization of macros and inline functions, but sometimes also
foregoes a note that would have been useful.<br>
<br></li>
<li>Since accurate source information is not available for non-list
forms, there is an element of heuristic in determining whether or
not to give a note about an atom. Spurious notes may be given when
a macro or inline function defines a variable that is also present
in the calling function. Notes about <tt class="code">nil</tt> and
<tt class="code">t</tt> are never given, since it is too easy to
confuse these constants in expanded code with ones in the original
source.<br>
<br></li>
<li>Notes are only given about code unreachable due to control
flow. There is no note when an expression is deleted because its
value is unused, since this is a common consequence of other
optimizations.</li>
</ul>
Somewhat spurious unreachable code notes can also result when a
macro inserts multiple copies of its arguments in different
contexts, for example:
<blockquote class="lisp">
<pre>
(defmacro t-and-f (var form)
  `(if ,var ,form ,form))

(defun foo (x)
  (t-and-f x (if x "True." "False.")))
</pre></blockquote>
results in these notes:
<blockquote class="example">
<pre>
In: DEFUN FOO
  (IF X "True." "False.")
==&gt;
  "False."
Note: Deleting unreachable code.

==&gt;
  "True."
Note: Deleting unreachable code.
</pre></blockquote>
It seems like it has deleted both branches of the <tt class=
"code">if</tt>, but it has really deleted one branch in one copy,
and the other branch in the other copy. Note that these messages
are only spurious in not satisfying the intent of the rule that
notes are only given when the deleted code appears in the original
source; there is always <tt class="variable">some</tt> code being
deleted when a unreachable code note is printed.<br>
<br>
<a name="toc164" id="toc164"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc157" id="htoc157"><b><font size=
"4">5.4.6</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Multiple Values
Optimization</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept179"></a> <a name="@concept180"></a><br>
Within a function, Python implements uses of multiple values
particularly efficiently. Multiple values can be kept in arbitrary
registers, so using multiple values doesn't imply stack
manipulation and representation conversion. For example, this code:
<blockquote class="example">
<pre>
(let ((a (if x (foo x) u))
      (b (if x (bar x) v)))
  ...)
</pre></blockquote>
is actually more efficient written this way:
<blockquote class="example">
<pre>
(multiple-value-bind
    (a b)
    (if x
        (values (foo x) (bar x))
        (values u v))
  ...)
</pre></blockquote>
Also, see section&nbsp;<a href="#local-call-return">5.6.5</a> for
information on how local call provides efficient support for
multiple function return values.<br>
<br>
<a name="toc165" id="toc165"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc158" id="htoc158"><b><font size=
"4">5.4.7</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Source to Source
Transformation</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept181"></a> <a name="@concept182"></a><br>
The compiler implements a number of operation-specific
optimizations as source-to-source transformations. You will often
see unfamiliar code in error messages, for example:
<blockquote class="lisp">
<pre>
(defun my-zerop () (zerop x))
</pre></blockquote>
gives this warning:
<blockquote class="example">
<pre>
In: DEFUN MY-ZEROP
  (ZEROP X)
==&gt;
  (= X 0)
Warning: Undefined variable: X
</pre></blockquote>
The original <tt class="code">zerop</tt> has been transformed into
a call to <tt class="code">=</tt>. This transformation is indicated
with the same <tt class="code">==&gt;</tt> used to mark macro and
function inline expansion. Although it can be confusing, display of
the transformed source is important, since warnings are given with
respect to the transformed source. This a more obscure example:
<blockquote class="lisp">
<pre>
(defun foo (x) (logand 1 x))
</pre></blockquote>
gives this efficiency note:
<blockquote class="example">
<pre>
In: DEFUN FOO
  (LOGAND 1 X)
==&gt;
  (LOGAND C::Y C::X)
Note: Forced to do static-function Two-arg-and (cost 53).
      Unable to do inline fixnum arithmetic (cost 1) because:
      The first argument is a INTEGER, not a FIXNUM.
      etc.
</pre></blockquote>
Here, the compiler commuted the call to <tt class=
"code">logand</tt>, introducing temporaries. The note complains
that the <tt class="variable">first</tt> argument is not a
<tt class="code">fixnum</tt>, when in the original call, it was the
second argument. To make things more confusing, the compiler
introduced temporaries called <tt class="code">c::x</tt> and
<tt class="code">c::y</tt> that are bound to <tt class=
"code">y</tt> and <tt class="code">1</tt>, respectively.<br>
<br>
You will also notice source-to-source optimizations when efficiency
notes are enabled (see section&nbsp;<a href=
"#efficiency-notes">5.13</a>.) When the compiler is unable to do a
transformation that might be possible if there was more
information, then an efficiency note is printed. For example,
<tt class="code">my-zerop</tt> above will also give this efficiency
note:
<blockquote class="example">
<pre>
In: DEFUN FOO
  (ZEROP X)
==&gt;
  (= X 0)
Note: Unable to optimize because:
      Operands might not be the same type, so can't open code.
</pre></blockquote>
<a name="toc166" id="toc166"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc159" id="htoc159"><b><font size=
"4">5.4.8</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Style
Recommendations</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept183"></a><br>
Source level optimization makes possible a clearer and more relaxed
programming style:
<ul>
<li>Don't use macros purely to avoid function call. If you want an
inline function, write it as a function and declare it inline. It's
clearer, less error-prone, and works just as well.<br>
<br></li>
<li>Don't write macros that try to ``optimize'' their expansion in
trivial ways such as avoiding binding variables for simple
expressions. The compiler does these optimizations too, and is less
likely to make a mistake.<br>
<br></li>
<li>Make use of local functions (i.e., <tt class="code">labels</tt>
or <tt class="code">flet</tt>) and tail-recursion in places where
it is clearer. Local function call is faster than full call.<br>
<br></li>
<li>Avoid setting local variables when possible. Binding a new
<tt class="code">let</tt> variable is at least as efficient as
setting an existing variable, and is easier to understand, both for
the compiler and the programmer.<br>
<br></li>
<li>Instead of writing similar code over and over again so that it
can be hand customized for each use, define a macro or inline
function, and let the compiler do the work.</li>
</ul>
<a name="toc167" id="toc167"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFD3">
<div align="center">
<table>
<tr>
<td><a name="htoc160" id="htoc160"><b><font size=
"5">5.5</font></b></a></td>
<td width="100%" align="center"><b><font size="5">Tail
Recursion</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="tail-recursion" id="tail-recursion"></a> <a name=
"@concept184"></a> <a name="@concept185"></a><br>
A call is tail-recursive if nothing has to be done after the the
call returns, i.e. when the call returns, the returned value is
immediately returned from the calling function. In this example,
the recursive call to <tt class="code">myfun</tt> is
tail-recursive:
<blockquote class="lisp">
<pre>
(defun myfun (x)
  (if (oddp (random x))
      (isqrt x)
      (myfun (1- x))))
</pre></blockquote>
Tail recursion is interesting because it is form of recursion that
can be implemented much more efficiently than general recursion. In
general, a recursive call requires the compiler to allocate storage
on the stack at run-time for every call that has not yet returned.
This memory consumption makes recursion unacceptably inefficient
for representing repetitive algorithms having large or unbounded
size. Tail recursion is the special case of recursion that is
semantically equivalent to the iteration constructs normally used
to represent repetition in programs. Because tail recursion is
equivalent to iteration, tail-recursive programs can be compiled as
efficiently as iterative programs.<br>
<br>
So why would you want to write a program recursively when you can
write it using a loop? Well, the main answer is that recursion is a
more general mechanism, so it can express some solutions simply
that are awkward to write as a loop. Some programmers also feel
that recursion is a stylistically preferable way to write loops
because it avoids assigning variables. For example, instead of
writing:
<blockquote class="lisp">
<pre>
(defun fun1 (x)
  something-that-uses-x)

(defun fun2 (y)
  something-that-uses-y)

(do ((x something (fun2 (fun1 x))))
    (nil))
</pre></blockquote>
You can write:
<blockquote class="lisp">
<pre>
(defun fun1 (x)
  (fun2 something-that-uses-x))

(defun fun2 (y)
  (fun1 something-that-uses-y))

(fun1 something)
</pre></blockquote>
The tail-recursive definition is actually more efficient, in
addition to being (arguably) clearer. As the number of functions
and the complexity of their call graph increases, the simplicity of
using recursion becomes compelling. Consider the advantages of
writing a large finite-state machine with separate tail-recursive
functions instead of using a single huge <tt class=
"code">prog</tt>.<br>
<br>
It helps to understand how to use tail recursion if you think of a
tail-recursive call as a <tt class="code">psetq</tt> that assigns
the argument values to the called function's variables, followed by
a <tt class="code">go</tt> to the start of the called function.
This makes clear an inherent efficiency advantage of tail-recursive
call: in addition to not having to allocate a stack frame, there is
no need to prepare for the call to return (e.g., by computing a
return PC.)<br>
<br>
Is there any disadvantage to tail recursion? Other than an increase
in efficiency, the only way you can tell that a call has been
compiled tail-recursively is if you use the debugger. Since a
tail-recursive call has no stack frame, there is no way the
debugger can print out the stack frame representing the call. The
effect is that backtrace will not show some calls that would have
been displayed in a non-tail-recursive implementation. In practice,
this is not as bad as it sounds---in fact it isn't really clearly
worse, just different. See section&nbsp;<a href=
"debugger.html#debug-tail-recursion">3.3.5</a> for information
about the debugger implications of tail recursion, and how to turn
it off for the sake of more conservative backtrace information.<br>
<br>
In order to ensure that tail-recursion is preserved in arbitrarily
complex calling patterns across separately compiled functions, the
compiler must compile any call in a tail-recursive position as a
tail-recursive call. This is done regardless of whether the program
actually exhibits any sort of recursive calling pattern. In this
example, the call to <tt class="code">fun2</tt> will always be
compiled as a tail-recursive call:
<blockquote class="lisp">
<pre>
(defun fun1 (x)
  (fun2 x))
</pre></blockquote>
So tail recursion doesn't necessarily have anything to do with
recursion as it is normally thought of. See section&nbsp;<a href=
"#local-tail-recursion">5.6.4</a> for more discussion of using tail
recursion to implement loops.<br>
<br>
<a name="toc168" id="toc168"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc161" id="htoc161"><b><font size=
"4">5.5.1</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Tail Recursion
Exceptions</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<br>
Although Python is claimed to be ``properly'' tail-recursive, some
might dispute this, since there are situations where tail recursion
is inhibited:
<ul>
<li>When the call is enclosed by a special binding, or<br>
<br></li>
<li>When the call is enclosed by a <tt class="code">catch</tt> or
<tt class="code">unwind-protect</tt>, or<br>
<br></li>
<li>When the call is enclosed by a <tt class="code">block</tt> or
<tt class="code">tagbody</tt> and the block name or <tt class=
"code">go</tt> tag has been closed over.</li>
</ul>
These dynamic extent binding forms inhibit tail recursion because
they allocate stack space to represent the binding. Shallow-binding
implementations of dynamic scoping also require cleanup code to be
evaluated when the scope is exited.<br>
<br>
In addition, optimization of tail-recursive calls is inhibited when
the <tt class="code">debug</tt> optimization quality is greater
than <tt class="code">2</tt> (see section&nbsp;<a href=
"debugger.html#debugger-policy">3.6</a>.)<br>
<br>
<a name="toc169" id="toc169"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFD3">
<div align="center">
<table>
<tr>
<td><a name="htoc162" id="htoc162"><b><font size=
"5">5.6</font></b></a></td>
<td width="100%" align="center"><b><font size="5">Local
Call</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="local-call" id="local-call"></a> <a name=
"@concept186"></a> <a name="@concept187"></a> <a name=
"@concept188"></a><br>
Python supports two kinds of function call: full call and local
call. Full call is the standard calling convention; its late
binding and generality make Common Lisp what it is, but create
unavoidable overheads. When the compiler can compile the calling
function and the called function simultaneously, it can use local
call to avoid some of the overhead of full call. Local call is
really a collection of compilation strategies. If some aspect of
call overhead is not needed in a particular local call, then it can
be omitted. In some cases, local call can be totally free. Local
call provides two main advantages to the user:
<ul>
<li>Local call makes the use of the lexical function binding forms
<a name="@funs128"></a><tt class="code">flet</tt> and <a name=
"@funs129"></a><tt class="code">labels</tt> much more efficient. A
local call is always faster than a full call, and in many cases is
much faster.<br>
<br></li>
<li>Local call is a natural approach to <i>block compilation</i>, a
compilation technique that resolves function references at compile
time. Block compilation speeds function call, but increases
compilation times and prevents function redefinition.</li>
</ul>
<a name="toc170" id="toc170"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc163" id="htoc163"><b><font size=
"4">5.6.1</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Self-Recursive
Calls</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept189"></a><br>
Local call is used when a function defined by <tt class=
"code">defun</tt> calls itself. For example:
<blockquote class="lisp">
<pre>
(defun fact (n)
  (if (zerop n)
      1
      (* n (fact (1- n)))))
</pre></blockquote>
This use of local call speeds recursion, but can also complicate
debugging, since <a name="@funs130"></a><tt class="code">trace</tt>
will only show the first call to <tt class="code">fact</tt>, and
not the recursive calls. This is because the recursive calls
directly jump to the start of the function, and don't indirect
through the <tt class="code">symbol-function</tt>. Self-recursive
local call is inhibited when the <tt class=
"code">:block-compile</tt> argument to <tt class=
"code">compile-file</tt> is <tt class="code">nil</tt> (see
section&nbsp;<a href="#compile-file-block">5.7.3</a>.)<br>
<br>
<a name="toc171" id="toc171"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc164" id="htoc164"><b><font size=
"4">5.6.2</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Let
Calls</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="let-calls" id="let-calls"></a> Because local call avoids
unnecessary call overheads, the compiler internally uses local call
to implement some macros and special forms that are not normally
thought of as involving a function call. For example, this
<tt class="code">let</tt>:
<blockquote class="example">
<pre>
(let ((a (foo))
      (b (bar)))
  ...)
</pre></blockquote>
is internally represented as though it was macroexpanded into:
<blockquote class="example">
<pre>
(funcall #'(lambda (a b)
             ...)
         (foo)
         (bar))
</pre></blockquote>
This implementation is acceptable because the simple cases of local
call (equivalent to a <tt class="code">let</tt>) result in good
code. This doesn't make <tt class="code">let</tt> any more
efficient, but does make local calls that are semantically the same
as <tt class="code">let</tt> much more efficient than full calls.
For example, these definitions are all the same as far as the
compiler is concerned:
<blockquote class="example">
<pre>
(defun foo ()
  ...some other stuff...
  (let ((a something))
    ...some stuff...))

(defun foo ()
  (flet ((localfun (a)
           ...some stuff...))
    ...some other stuff...
    (localfun something)))

(defun foo ()
  (let ((funvar #'(lambda (a)
                    ...some stuff...)))
    ...some other stuff...
    (funcall funvar something)))
</pre></blockquote>
Although local call is most efficient when the function is called
only once, a call doesn't have to be equivalent to a <tt class=
"code">let</tt> to be more efficient than full call. All local
calls avoid the overhead of argument count checking and keyword
argument parsing, and there are a number of other advantages that
apply in many common situations. See section&nbsp;<a href=
"#let-optimization">5.4.1</a> for a discussion of the optimizations
done on let calls.<br>
<br>
<a name="toc172" id="toc172"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc165" id="htoc165"><b><font size=
"4">5.6.3</font></b></a></td>
<td width="100%" align="center"><b><font size=
"4">Closures</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept190"></a><br>
Local call allows for much more efficient use of closures, since
the closure environment doesn't need to be allocated on the heap,
or even stored in memory at all. In this example, there is no
penalty for <tt class="code">localfun</tt> referencing <tt class=
"code">a</tt> and <tt class="code">b</tt>:
<blockquote class="lisp">
<pre>
(defun foo (a b)
  (flet ((localfun (x)
           (1+ (* a b x))))
    (if (= a b)
        (localfun (- x))
        (localfun x))))
</pre></blockquote>
In local call, the compiler effectively passes closed-over values
as extra arguments, so there is no need for you to ``optimize''
local function use by explicitly passing in lexically visible
values. Closures may also be subject to let optimization (see
section&nbsp;<a href="#let-optimization">5.4.1</a>.)<br>
<br>
Note: indirect value cells are currently always allocated on the
heap when a variable is both assigned to (with <tt class=
"code">setq</tt> or <tt class="code">setf</tt>) and closed over,
regardless of whether the closure is a local function or not. This
is another reason to avoid setting variables when you don't have
to.<br>
<br>
<a name="toc173" id="toc173"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc166" id="htoc166"><b><font size=
"4">5.6.4</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Local Tail
Recursion</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="local-tail-recursion" id="local-tail-recursion"></a>
<a name="@concept191"></a> <a name="@concept192"></a><br>
Tail-recursive local calls are particularly efficient, since they
are in effect an assignment plus a control transfer. Scheme
programmers write loops with tail-recursive local calls, instead of
using the imperative <tt class="code">go</tt> and <tt class=
"code">setq</tt>. This has not caught on in the Common Lisp
community, since conventional Common Lisp compilers don't implement
local call. In Python, users can choose to write loops such as:
<blockquote class="lisp">
<pre>
(defun ! (n)
  (labels ((loop (n total)
             (if (zerop n)
                 total
                 (loop (1- n) (* n total)))))
    (loop n 1)))
</pre></blockquote>
<br>
<a name="@funs131"></a><a name="FN:iterate" id="FN:iterate"></a>
<div align="left">[Macro]<br>
<tt class="function-name">extensions:</tt><tt class=
"function-name">iterate</tt> <tt class="variable">name</tt>
(<tt class="code">{(<tt class="variable">var</tt> <tt class=
"variable">initial-value</tt>)}</tt><sup><font size=
"2">*</font></sup>) <tt class="code">{<tt class=
"variable">declaration</tt>}</tt><sup><font size="2">*</font></sup>
<tt class="code">{<tt class=
"variable">form</tt>}</tt><sup><font size="2">*</font></sup>
&nbsp;&nbsp;&nbsp;</div>
<blockquote><br>
This macro provides syntactic sugar for using <a name=
"@funs132"></a><tt class="code">labels</tt> to do iteration. It
creates a local function <tt class="variable">name</tt> with the
specified <tt class="variable">var</tt>s as its arguments and the
<tt class="variable">declaration</tt>s and <tt class=
"variable">form</tt>s as its body. This function is then called
with the <tt class="variable">initial-values</tt>, and the result
of the call is return from the macro.<br>
<br>
Here is our factorial example rewritten using <tt class=
"code">iterate</tt>:
<blockquote class="lisp">
<pre>
    (defun ! (n)
      (iterate loop
               ((n n)
               (total 1))
        (if (zerop n)
          total
          (loop (1- n) (* n total)))))
  
</pre></blockquote>
The main advantage of using <tt class="code">iterate</tt> over
<tt class="code">do</tt> is that <tt class="code">iterate</tt>
naturally allows stepping to be done differently depending on
conditionals in the body of the loop. <tt class="code">iterate</tt>
can also be used to implement algorithms that aren't really
iterative by simply doing a non-tail call. For example, the
standard recursive definition of factorial can be written like
this:
<blockquote class="lisp">
<pre>
(iterate fact
         ((n n))
  (if (zerop n)
      1
      (* n (fact (1- n)))))
</pre></blockquote>
</blockquote>
<a name="toc174" id="toc174"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc167" id="htoc167"><b><font size=
"4">5.6.5</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Return
Values</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="local-call-return" id="local-call-return"></a> <a name=
"@concept193"></a> <a name="@concept194"></a><br>
One of the more subtle costs of full call comes from allowing
arbitrary numbers of return values. This overhead can be avoided in
local calls to functions that always return the same number of
values. For efficiency reasons (as well as stylistic ones), you
should write functions so that they always return the same number
of values. This may require passing extra <tt class="code">nil</tt>
arguments to <tt class="code">values</tt> in some cases, but the
result is more efficient, not less so.<br>
<br>
When efficiency notes are enabled (see section&nbsp;<a href=
"#efficiency-notes">5.13</a>), and the compiler wants to use known
values return, but can't prove that the function always returns the
same number of values, then it will print a note like this:
<blockquote class="example">
<pre>
In: DEFUN GRUE
  (DEFUN GRUE (X) (DECLARE (FIXNUM X)) (COND (# #) (# NIL) (T #)))
Note: Return type not fixed values, so can't use known return convention:
  (VALUES (OR (INTEGER -536870912 -1) NULL) &amp;REST T)
</pre></blockquote>
In order to implement proper tail recursion in the presence of
known values return (see section&nbsp;<a href=
"#tail-recursion">5.5</a>), the compiler sometimes must prove that
multiple functions all return the same number of values. When this
can't be proven, the compiler will print a note like this:
<blockquote class="example">
<pre>
In: DEFUN BLUE
  (DEFUN BLUE (X) (DECLARE (FIXNUM X)) (COND (# #) (# #) (# #) (T #)))
Note: Return value count mismatch prevents known return from
      these functions:
  BLUE
  SNOO
</pre></blockquote>
See section&nbsp;<a href="#number-local-call">5.11.10</a> for the
interaction between local call and the representation of numeric
types.<br>
<br>
<a name="toc175" id="toc175"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFD3">
<div align="center">
<table>
<tr>
<td><a name="htoc168" id="htoc168"><b><font size=
"5">5.7</font></b></a></td>
<td width="100%" align="center"><b><font size="5">Block
Compilation</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="block-compilation" id="block-compilation"></a> <a name=
"@concept195"></a> <a name="@concept196"></a><br>
Block compilation allows calls to global functions defined by
<a name="@funs133"></a><tt class="code">defun</tt> to be compiled
as local calls. The function call can be in a different top-level
form than the <tt class="code">defun</tt>, or even in a different
file.<br>
<br>
In addition, block compilation allows the declaration of the
<i>entry points</i> to the block compiled portion. An entry point
is any function that may be called from outside of the block
compilation. If a function is not an entry point, then it can be
compiled more efficiently, since all calls are known at compile
time. In particular, if a function is only called in one place,
then it will be let converted. This effectively inline expands the
function, but without the code duplication that results from
defining the function normally and then declaring it inline.<br>
<br>
The main advantage of block compilation is that it it preserves
efficiency in programs even when (for readability and syntactic
convenience) they are broken up into many small functions. There is
absolutely no overhead for calling a non-entry point function that
is defined purely for modularity (i.e. called only in one
place.)<br>
<br>
Block compilation also allows the use of non-descriptor arguments
and return values in non-trivial programs (see
section&nbsp;<a href="#number-local-call">5.11.10</a>).<br>
<br>
<a name="toc176" id="toc176"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc169" id="htoc169"><b><font size=
"4">5.7.1</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Block Compilation
Semantics</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<br>
The effect of block compilation can be envisioned as the compiler
turning all the <tt class="code">defun</tt>s in the block
compilation into a single <tt class="code">labels</tt> form:
<blockquote class="example">
<pre>
(declaim (start-block fun1 fun3))

(defun fun1 ()
  ...)

(defun fun2 ()
  ...
  (fun1)
  ...)

(defun fun3 (x)
  (if x
      (fun1)
      (fun2)))

(declaim (end-block))
</pre></blockquote>
becomes:
<blockquote class="example">
<pre>
(labels ((fun1 ()
           ...)
         (fun2 ()
           ...
           (fun1)
           ...)
         (fun3 (x)
           (if x
               (fun1)
               (fun2))))
  (setf (fdefinition 'fun1) #'fun1)
  (setf (fdefinition 'fun3) #'fun3))
</pre></blockquote>
Calls between the block compiled functions are local calls, so
changing the global definition of <tt class="code">fun1</tt> will
have no effect on what <tt class="code">fun2</tt> does; <tt class=
"code">fun2</tt> will keep calling the old <tt class=
"code">fun1</tt>.<br>
<br>
The entry points <tt class="code">fun1</tt> and <tt class=
"code">fun3</tt> are still installed in the <tt class=
"code">symbol-function</tt> as the global definitions of the
functions, so a full call to an entry point works just as before.
However, <tt class="code">fun2</tt> is not an entry point, so it is
not globally defined. In addition, <tt class="code">fun2</tt> is
only called in one place, so it will be let converted.<br>
<br>
<a name="toc177" id="toc177"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc170" id="htoc170"><b><font size=
"4">5.7.2</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Block Compilation
Declarations</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept197"></a> <a name="@concept198"></a> <a name=
"@concept199"></a><br>
The <tt class="code">extensions:start-block</tt> and <tt class=
"code">extensions:end-block</tt> declarations allow fine-grained
control of block compilation. These declarations are only legal as
a global declarations (<tt class="code">declaim</tt> or <tt class=
"code">proclaim</tt>).<br>
<br>
<br>
The <tt class="code">start-block</tt> declaration has this syntax:
<blockquote class="example">
<pre>
(start-block <tt class="code">{<tt class=
"variable">entry-point-name</tt>}</tt><sup><font size=
"2">*</font></sup>)
</pre></blockquote>
When processed by the compiler, this declaration marks the start of
block compilation, and specifies the entry points to that block. If
no entry points are specified, then <tt class="variable">all</tt>
functions are made into entry points. If already block compiling,
then the compiler ends the current block and starts a new one.<br>
<br>
<br>
The <tt class="code">end-block</tt> declaration has no arguments:
<blockquote class="lisp">
<pre>
(end-block)
</pre></blockquote>
The <tt class="code">end-block</tt> declaration ends a block
compilation unit without starting a new one. This is useful mainly
when only a portion of a file is worth block compiling.<br>
<br>
<a name="toc178" id="toc178"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc171" id="htoc171"><b><font size=
"4">5.7.3</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Compiler
Arguments</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="compile-file-block" id="compile-file-block"></a> <a name=
"@concept200"></a><br>
The <tt class="code">:block-compile</tt> and <tt class=
"code">:entry-points</tt> arguments to <tt class=
"code">extensions:compile-from-stream</tt> and <a name=
"@funs134"></a><tt class="code">compile-file</tt> provide overall
control of block compilation, and allow block compilation without
requiring modification of the program source.<br>
<br>
There are three possible values of the <tt class=
"code">:block-compile</tt> argument:
<dl compact="compact">
<dt><tt class="code">nil</tt><br></dt>
<dd>Do no compile-time resolution of global function names, not
even for self-recursive calls. This inhibits any <tt class=
"code">start-block</tt> declarations appearing in the file,
allowing all functions to be incrementally redefined.<br>
<br></dd>
<dt><tt class="code">t</tt><br></dt>
<dd>Start compiling in block compilation mode. This is mainly
useful for block compiling small files that contain no <tt class=
"code">start-block</tt> declarations. See also the <tt class=
"code">:entry-points</tt> argument.<br>
<br></dd>
<dt><tt class="code">:specified</tt><br></dt>
<dd>Start compiling in form-at-a-time mode, but exploit any
<tt class="code">start-block</tt> declarations and compile
self-recursive calls as local calls. Normally <tt class=
"code">:specified</tt> is the default for this argument (see
<a name="@vars48"></a><tt class=
"code">*block-compile-default*</tt>.)</dd>
</dl>
The <tt class="code">:entry-points</tt> argument can be used in
conjunction with <tt class="code">:block-compile</tt> <tt class=
"code">t</tt> to specify the entry-points to a block-compiled file.
If not specified or <tt class="code">nil</tt>, all global functions
will be compiled as entry points. When <tt class=
"code">:block-compile</tt> is not <tt class="code">t</tt>, this
argument is ignored.<br>
<br>
<br>
<a name="@vars49"></a><a name="VR:block-compile-default" id=
"VR:block-compile-default"></a>
<div align="left">[Variable]<br>
<tt class="function-name">*block-compile-default*</tt>
&nbsp;&nbsp;&nbsp;</div>
<blockquote><br>
This variable determines the default value for the <tt class=
"code">:block-compile</tt> argument to <tt class=
"code">compile-file</tt> and <tt class=
"code">compile-from-stream</tt>. The initial value of this variable
is <tt class="code">:specified</tt>, but <tt class="code">nil</tt>
is sometimes useful for totally inhibiting block
compilation.</blockquote>
<a name="toc179" id="toc179"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc172" id="htoc172"><b><font size=
"4">5.7.4</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Practical
Difficulties</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<br>
The main problem with block compilation is that the compiler uses
large amounts of memory when it is block compiling. This places an
upper limit on the amount of code that can be block compiled as a
unit. To make best use of block compilation, it is necessary to
locate the parts of the program containing many internal calls, and
then add the appropriate <tt class="code">start-block</tt>
declarations. When writing new code, it is a good idea to put in
block compilation declarations from the very beginning, since
writing block declarations correctly requires accurate knowledge of
the program's function call structure. If you want to initially
develop code with full incremental redefinition, you can compile
with <a name="@vars50"></a><tt class=
"code">*block-compile-default*</tt> set to <tt class=
"code">nil</tt>.<br>
<br>
Note if a <tt class="code">defun</tt> appears in a non-null lexical
environment, then calls to it cannot be block compiled.<br>
<br>
Unless files are very small, it is probably impractical to block
compile multiple files as a unit by specifying a list of files to
<tt class="code">compile-file</tt>. Semi-inline expansion (see
section&nbsp;<a href="#semi-inline">5.8.2</a>) provides another way
to extend block compilation across file boundaries.<br>
<br>
<a name="toc180" id="toc180"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc173" id="htoc173"><b><font size=
"4">5.7.5</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Context
Declarations</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="context-declarations" id="context-declarations"></a>
<a name="@concept201"></a> <a name="@concept202"></a><br>
CMUCL has a context-sensitive declaration mechanism which is useful
because it allows flexible control of the compilation policy in
large systems without requiring changes to the source files. The
primary use of this feature is to allow the exported interfaces of
a system to be compiled more safely than the system internals. The
context used is the name being defined and the kind of definition
(function, macro, etc.)<br>
<br>
The <tt class="code">:context-declarations</tt> option to <a name=
"@funs135"></a><tt class="code">with-compilation-unit</tt> has
dynamic scope, affecting all compilation done during the evaluation
of the body. The argument to this option should evaluate to a list
of lists of the form:
<blockquote class="example">
<pre>
(<tt class="variable">context-spec</tt> <tt class=
"code">{<tt class="variable">declare-form</tt>}</tt><sup><font size="2">+</font></sup>)
</pre></blockquote>
In the indicated context, the specified declare forms are inserted
at the head of each definition. The declare forms for all contexts
that match are appended together, with earlier declarations getting
precedence over later ones. A simple example:
<blockquote class="example">
<pre>
    :context-declarations
    '((:external (declare (optimize (safety 2)))))
</pre></blockquote>
This will cause all functions that are named by external symbols to
be compiled with <tt class="code">safety 2</tt>.<br>
<br>
The full syntax of context specs is:
<dl compact="compact">
<dt><tt class="code">:internal</tt>, <tt class=
"code">:external</tt><br></dt>
<dd>True if the symbol is internal (external) in its home
package.<br>
<br></dd>
<dt><tt class="code">:uninterned</tt><br></dt>
<dd>True if the symbol has no home package.<br>
<br></dd>
<dt><tt class="code">(:package <tt class="code">{<tt class=
"variable">package-name</tt>}</tt><sup><font size=
"2">*</font></sup>)</tt><br></dt>
<dd>True if the symbol's home package is in any of the named
packages (false if uninterned.)<br>
<br></dd>
<dt><tt class="code">:anonymous</tt><br></dt>
<dd>True if the function doesn't have any interesting name (not
<tt class="code">defmacro</tt>, <tt class="code">defun</tt>,
<tt class="code">labels</tt> or <tt class="code">flet</tt>).<br>
<br></dd>
<dt><tt class="code">:macro</tt>, <tt class=
"code">:function</tt><br></dt>
<dd><tt class="code">:macro</tt> is a global (<tt class=
"code">defmacro</tt>) macro. <tt class="code">:function</tt> is
anything else.<br>
<br></dd>
<dt><tt class="code">:local</tt>, <tt class=
"code">:global</tt><br></dt>
<dd><tt class="code">:local</tt> is a <tt class="code">labels</tt>
or <tt class="code">flet</tt>. <tt class="code">:global</tt> is
anything else.<br>
<br></dd>
<dt><tt class="code">(:or <tt class="code">{<tt class=
"variable">context-spec</tt>}</tt><sup><font size=
"2">*</font></sup>)</tt><br></dt>
<dd>True when any supplied <tt class="variable">context-spec</tt>
is true.<br>
<br></dd>
<dt><tt class="code">(:and <tt class="code">{<tt class=
"variable">context-spec</tt>}</tt><sup><font size=
"2">*</font></sup>)</tt><br></dt>
<dd>True only when all supplied <tt class=
"variable">context-spec</tt>s are true.<br>
<br></dd>
<dt><tt class="code">(:not <tt class="code">{<tt class=
"variable">context-spec</tt>}</tt><sup><font size=
"2">*</font></sup>)</tt><br></dt>
<dd>True when <tt class="variable">context-spec</tt> is false.<br>
<br></dd>
<dt><tt class="code">(:member <tt class="code">{<tt class=
"variable">name</tt>}</tt><sup><font size=
"2">*</font></sup>)</tt><br></dt>
<dd>True when the defined name is one of these names (<tt class=
"code">equal</tt> test.)<br>
<br></dd>
<dt><tt class="code">(:match <tt class="code">{<tt class=
"variable">pattern</tt>}</tt><sup><font size=
"2">*</font></sup>)</tt><br></dt>
<dd>True when any of the patterns is a substring of the name. The
name is wrapped with <tt class="code">$</tt>'s, so ``<tt class=
"code">$FOO</tt>'' matches names beginning with ``<tt class=
"code">FOO</tt>'', etc.</dd>
</dl>
<a name="toc181" id="toc181"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc174" id="htoc174"><b><font size=
"4">5.7.6</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Context
Declaration Example</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<br>
Here is a more complex example of <tt class=
"code">with-compilation-unit</tt> options:
<blockquote class="example">
<pre>
:optimize '(optimize (speed 2) (space 2) (inhibit-warnings 2)
                     (debug 1) (safety 0))
:optimize-interface '(optimize-interface (safety 1) (debug 1))
:context-declarations
'(((:or :external (:and (:match "%") (:match "SET")))
   (declare (optimize-interface (safety 2))))
  ((:or (:and :external :macro)
        (:match "$PARSE-"))
   (declare (optimize (safety 2)))))
</pre></blockquote>
The <tt class="code">optimize</tt> and <tt class=
"code">extensions:optimize-interface</tt> declarations (see
section&nbsp;<a href=
"compiler.html#optimize-declaration">4.7.1</a>) set up the global
compilation policy. The bodies of functions are to be compiled
completely unsafe (<tt class="code">safety 0</tt>), but argument
count and weakened argument type checking is to be done when a
function is called (<tt class="code">speed 2 safety 1</tt>).<br>
<br>
The first declaration specifies that all functions that are
external or whose names contain both ``<tt class="code">%</tt>''
and ``<tt class="code">SET</tt>'' are to be compiled compiled with
completely safe interfaces (<tt class="code">safety 2</tt>). The
reason for this particular <tt class="code">:match</tt> rule is
that <tt class="code">setf</tt> inverse functions in this system
tend to have both strings in their name somewhere. We want
<tt class="code">setf</tt> inverses to be safe because they are
implicitly called by users even though their name is not
exported.<br>
<br>
The second declaration makes external macros or functions whose
names start with ``<tt class="code">PARSE-</tt>'' have safe bodies
(as well as interfaces). This is desirable because a syntax error
in a macro may cause a type error inside the body. The <tt class=
"code">:match</tt> rule is used because macros often have auxiliary
functions whose names begin with this string.<br>
<br>
This particular example is used to build part of the standard CMUCL
system. Note however, that context declarations must be set up
according to the needs and coding conventions of a particular
system; different parts of CMUCL are compiled with different
context declarations, and your system will probably need its own
declarations. In particular, any use of the <tt class=
"code">:match</tt> option depends on naming conventions used in
coding.<br>
<br>
<a name="toc182" id="toc182"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFD3">
<div align="center">
<table>
<tr>
<td><a name="htoc175" id="htoc175"><b><font size=
"5">5.8</font></b></a></td>
<td width="100%" align="center"><b><font size="5">Inline
Expansion</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="inline-expansion" id="inline-expansion"></a> <a name=
"@concept203"></a> <a name="@concept204"></a> <a name=
"@concept205"></a> <a name="@concept206"></a> <a name=
"@concept207"></a><br>
Python can expand almost any function inline, including functions
with keyword arguments. The only restrictions are that keyword
argument keywords in the call must be constant, and that global
function definitions (<tt class="code">defun</tt>) must be done in
a null lexical environment (not nested in a <tt class=
"code">let</tt> or other binding form.) Local functions (<tt class=
"code">flet</tt>) can be inline expanded in any environment.
Combined with Python's source-level optimization, inline expansion
can be used for things that formerly required macros for efficient
implementation. In Python, macros don't have any efficiency
advantage, so they need only be used where a macro's syntactic
flexibility is required.<br>
<br>
Inline expansion is a compiler optimization technique that reduces
the overhead of a function call by simply not doing the call:
instead, the compiler effectively rewrites the program to appear as
though the definition of the called function was inserted at each
call site. In Common Lisp, this is straightforwardly expressed by
inserting the <tt class="code">lambda</tt> corresponding to the
original definition:
<blockquote class="lisp">
<pre>
(proclaim '(inline my-1+))
(defun my-1+ (x) (+ x 1))

(my-1+ someval) ==&gt; ((lambda (x) (+ x 1)) someval)
</pre></blockquote>
When the function expanded inline is large, the program after
inline expansion may be substantially larger than the original
program. If the program becomes too large, inline expansion hurts
speed rather than helping it, since hardware resources such as
physical memory and cache will be exhausted. Inline expansion is
called for:
<ul>
<li>When profiling has shown that a relatively simple function is
called so often that a large amount of time is being wasted in the
calling of that function (as opposed to running in that function.)
If a function is complex, it will take a long time to run relative
the time spent in call, so the speed advantage of inline expansion
is diminished at the same time the space cost of inline expansion
is increased. Of course, if a function is rarely called, then the
overhead of calling it is also insignificant.<br>
<br></li>
<li>With functions so simple that they take less space to inline
expand than would be taken to call the function (such as <tt class=
"code">my-1+</tt> above.) It would require intimate knowledge of
the compiler to be certain when inline expansion would reduce
space, but it is generally safe to inline expand functions whose
definition is a single function call, or a few calls to simple
Common Lisp functions.</li>
</ul>
In addition to this speed/space tradeoff from inline expansion's
avoidance of the call, inline expansion can also reveal
opportunities for optimization. Python's extensive source-level
optimization can make use of context information from the caller to
tremendously simplify the code resulting from the inline expansion
of a function.<br>
<br>
The main form of caller context is local information about the
actual argument values: what the argument types are and whether the
arguments are constant. Knowledge about argument types can
eliminate run-time type tests (e.g., for generic arithmetic.)
Constant arguments in a call provide opportunities for constant
folding optimization after inline expansion.<br>
<br>
A hidden way that constant arguments are often supplied to
functions is through the defaulting of unsupplied optional or
keyword arguments. There can be a huge efficiency advantage to
inline expanding functions that have complex keyword-based
interfaces, such as this definition of the <tt class=
"code">member</tt> function:
<blockquote class="lisp">
<pre>
(proclaim '(inline member))
(defun member (item list &amp;key
                    (key #'identity)
                    (test #'eql testp)
                    (test-not nil notp))
  (do ((list list (cdr list)))
      ((null list) nil)
    (let ((car (car list)))
      (if (cond (testp
                 (funcall test item (funcall key car)))
                (notp
                 (not (funcall test-not item (funcall key car))))
                (t
                 (funcall test item (funcall key car))))
          (return list)))))

</pre></blockquote>
After inline expansion, this call is simplified to the obvious
code:
<blockquote class="lisp">
<pre>
(member a l :key #'foo-a :test #'char=) ==&gt;

(do ((list list (cdr list)))
    ((null list) nil)
  (let ((car (car list)))
    (if (char= item (foo-a car))
        (return list))))
</pre></blockquote>
In this example, there could easily be more than an order of
magnitude improvement in speed. In addition to eliminating the
original call to <tt class="code">member</tt>, inline expansion
also allows the calls to <tt class="code">char=</tt> and <tt class=
"code">foo-a</tt> to be open-coded. We go from a loop with three
tests and two calls to a loop with one test and no calls.<br>
<br>
See section&nbsp;<a href="#source-optimization">5.4</a> for more
discussion of source level optimization.<br>
<br>
<a name="toc183" id="toc183"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc176" id="htoc176"><b><font size=
"4">5.8.1</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Inline Expansion
Recording</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept208"></a><br>
Inline expansion requires that the source for the inline expanded
function to be available when calls to the function are compiled.
The compiler doesn't remember the inline expansion for every
function, since that would take an excessive about of space.
Instead, the programmer must tell the compiler to record the inline
expansion before the definition of the inline expanded function is
compiled. This is done by globally declaring the function inline
before the function is defined, by using the <tt class=
"code">inline</tt> and <tt class=
"code">extensions:maybe-inline</tt> (see section&nbsp;<a href=
"#maybe-inline-declaration">5.8.3</a>) declarations.<br>
<br>
In addition to recording the inline expansion of inline functions
at the time the function is compiled, <tt class=
"code">compile-file</tt> also puts the inline expansion in the
output file. When the output file is loaded, the inline expansion
is made available for subsequent compilations; there is no need to
compile the definition again to record the inline expansion.<br>
<br>
If a function is declared inline, but no expansion is recorded,
then the compiler will give an efficiency note like:
<blockquote class="example">
<pre>
Note: MYFUN is declared inline, but has no expansion.
</pre></blockquote>
When you get this note, check that the <tt class="code">inline</tt>
declaration and the definition appear before the calls that are to
be inline expanded. This note will also be given if the inline
expansion for a <tt class="code">defun</tt> could not be recorded
because the <tt class="code">defun</tt> was in a non-null lexical
environment.<br>
<br>
<a name="toc184" id="toc184"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc177" id="htoc177"><b><font size=
"4">5.8.2</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Semi-Inline
Expansion</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="semi-inline" id="semi-inline"></a><br>
Python supports <tt class="variable">semi-inline</tt> functions.
Semi-inline expansion shares a single copy of a function across all
the calls in a component by converting the inline expansion into a
local function (see section&nbsp;<a href="#local-call">5.6</a>.)
This takes up less space when there are multiple calls, but also
provides less opportunity for context dependent optimization. When
there is only one call, the result is identical to normal inline
expansion. Semi-inline expansion is done when the <tt class=
"code">space</tt> optimization quality is <tt class="code">0</tt>,
and the function has been declared <tt class=
"code">extensions:maybe-inline</tt>.<br>
<br>
This mechanism of inline expansion combined with local call also
allows recursive functions to be inline expanded. If a recursive
function is declared <tt class="code">inline</tt>, calls will
actually be compiled semi-inline. Although recursive functions are
often so complex that there is little advantage to semi-inline
expansion, it can still be useful in the same sort of cases where
normal inline expansion is especially advantageous, i.e. functions
where the calling context can help a lot.<br>
<br>
<a name="toc185" id="toc185"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc178" id="htoc178"><b><font size=
"4">5.8.3</font></b></a></td>
<td width="100%" align="center"><b><font size="4">The Maybe-Inline
Declaration</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="maybe-inline-declaration" id=
"maybe-inline-declaration"></a> <a name="@concept209"></a><br>
The <tt class="code">extensions:maybe-inline</tt> declaration is a
CMUCL extension. It is similar to <tt class="code">inline</tt>, but
indicates that inline expansion may sometimes be desirable, rather
than saying that inline expansion should almost always be done.
When used in a global declaration, <tt class=
"code">extensions:maybe-inline</tt> causes the expansion for the
named functions to be recorded, but the functions aren't actually
inline expanded unless <tt class="code">space</tt> is <tt class=
"code">0</tt> or the function is eventually (perhaps locally)
declared <tt class="code">inline</tt>.<br>
<br>
Use of the <tt class="code">extensions:maybe-inline</tt>
declaration followed by the <tt class="code">defun</tt> is
preferable to the standard idiom of:
<blockquote class="lisp">
<pre>
(proclaim '(inline myfun))
(defun myfun () ...)
(proclaim '(notinline myfun))

;;; <i>Any calls to </i><tt class=
"code"><i>myfun</i></tt><i> here are not inline expanded.</i>

(defun somefun ()
  (declare (inline myfun))
  ;;
  ;; <i>Calls to </i><tt class=
"code"><i>myfun</i></tt><i> here are inline expanded.</i>
  ...)
</pre></blockquote>
The problem with using <tt class="code">notinline</tt> in this way
is that in Common Lisp it does more than just suppress inline
expansion, it also forbids the compiler to use any knowledge of
<tt class="code">myfun</tt> until a later <tt class=
"code">inline</tt> declaration overrides the <tt class=
"code">notinline</tt>. This prevents compiler warnings about
incorrect calls to the function, and also prevents block
compilation.<br>
<br>
The <tt class="code">extensions:maybe-inline</tt> declaration is
used like this:
<blockquote class="lisp">
<pre>
(proclaim '(extensions:maybe-inline myfun))
(defun myfun () ...)

;;; <i>Any calls to </i><tt class=
"code"><i>myfun</i></tt><i> here are not inline expanded.</i>

(defun somefun ()
  (declare (inline myfun))
  ;;
  ;; <i>Calls to </i><tt class=
"code"><i>myfun</i></tt><i> here are inline expanded.</i>
  ...)

(defun someotherfun ()
  (declare (optimize (space 0)))
  ;;
  ;; <i>Calls to </i><tt class=
"code"><i>myfun</i></tt><i> here are expanded semi-inline.</i>
  ...)
</pre></blockquote>
In this example, the use of <tt class=
"code">extensions:maybe-inline</tt> causes the expansion to be
recorded when the <tt class="code">defun</tt> for <tt class=
"code">somefun</tt> is compiled, and doesn't waste space through
doing inline expansion by default. Unlike <tt class=
"code">notinline</tt>, this declaration still allows the compiler
to assume that the known definition really is the one that will be
called when giving compiler warnings, and also allows the compiler
to do semi-inline expansion when the policy is appropriate.<br>
<br>
When the goal is merely to control whether inline expansion is done
by default, it is preferable to use <tt class=
"code">extensions:maybe-inline</tt> rather than <tt class=
"code">notinline</tt>. The <tt class="code">notinline</tt>
declaration should be reserved for those special occasions when a
function may be redefined at run-time, so the compiler must be told
that the obvious definition of a function is not necessarily the
one that will be in effect at the time of the call.<br>
<br>
<a name="toc186" id="toc186"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFD3">
<div align="center">
<table>
<tr>
<td><a name="htoc179" id="htoc179"><b><font size=
"5">5.9</font></b></a></td>
<td width="100%" align="center"><b><font size="5">Byte Coded
Compilation</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="byte-compile" id="byte-compile"></a> <a name=
"@concept210"></a> <a name="@concept211"></a><br>
Python supports byte compilation to reduce the size of Lisp
programs by allowing functions to be compiled more compactly. Byte
compilation provides an extreme speed/space tradeoff: byte code is
typically six times more compact than native code, but runs fifty
times (or more) slower. This is about ten times faster than the
standard interpreter, which is itself considered fast in comparison
to other Common Lisp interpreters.<br>
<br>
Large Lisp systems (such as CMUCL itself) often have large amounts
of user-interface code, compile-time (macro) code, debugging code,
or rarely executed special-case code. This code is a good target
for byte compilation: very little time is spent running in it, but
it can take up quite a bit of space. Straight-line code with many
function calls is much more suitable than inner loops.<br>
<br>
When byte-compiling, the compiler compiles about twice as fast, and
can produce a hardware independent object file (<tt class=
"filename">.bytef</tt> type.) This file can be loaded like a normal
fasl file on any implementation of CMUCL with the same
byte-ordering.<br>
<br>
The decision to byte compile or native compile can be done on a
per-file or per-code-object basis. The <tt class=
"code">:byte-compile</tt> argument to <a name=
"@funs136"></a><tt class="code">compile-file</tt> has these
possible values:<br>
<br>
<dl compact="compact">
<dt><tt class="code">nil</tt><br></dt>
<dd>Don't byte compile anything in this file.<br>
<br></dd>
<dt><tt class="code">t</tt><br></dt>
<dd>Byte compile everything in this file and produce a
processor-independent <tt class="filename">.bytef</tt> file.<br>
<br></dd>
<dt><tt class="code">:maybe</tt><br></dt>
<dd>Produce a normal fasl file, but byte compile any functions for
which the <tt class="code">speed</tt> optimization quality is
<tt class="code">0</tt> and the <tt class="code">debug</tt> quality
is not greater than <tt class="code">1</tt>.</dd>
</dl>
<br>
<a name="@vars51"></a><a name="VR:byte-compile-top-level" id=
"VR:byte-compile-top-level"></a>
<div align="left">[Variable]<br>
<tt class="function-name">extensions:</tt><tt class=
"function-name">*byte-compile-top-level*</tt>
&nbsp;&nbsp;&nbsp;</div>
<blockquote><br>
If this variable is true (the default) and the <tt class=
"code">:byte-compile</tt> argument to <tt class=
"code">compile-file</tt> is <tt class="code">:maybe</tt>, then byte
compile top-level code (code outside of any <tt class=
"code">defun</tt>, <tt class="code">defmethod</tt>,
etc.)</blockquote>
<br>
<a name="@vars52"></a><a name="VR:byte-compile-default" id=
"VR:byte-compile-default"></a>
<div align="left">[Variable]<br>
<tt class="function-name">extensions:</tt><tt class=
"function-name">*byte-compile-default*</tt>
&nbsp;&nbsp;&nbsp;</div>
<blockquote><br>
This variable determines the default value for the <tt class=
"code">:byte-compile</tt> argument to <tt class=
"code">compile-file</tt>, initially <tt class=
"code">:maybe</tt>.</blockquote>
<a name="toc187" id="toc187"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFD3">
<div align="center">
<table>
<tr>
<td><a name="htoc180" id="htoc180"><b><font size=
"5">5.10</font></b></a></td>
<td width="100%" align="center"><b><font size="5">Object
Representation</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="object-representation" id="object-representation"></a>
<a name="@concept212"></a> <a name="@concept213"></a> <a name=
"@concept214"></a><br>
A somewhat subtle aspect of writing efficient Common Lisp programs
is choosing the correct data structures so that the underlying
objects can be implemented efficiently. This is partly because of
the need for multiple representations for a given value (see
section&nbsp;<a href="#non-descriptor">5.11.2</a>), but is also due
to the sheer number of object types that Common Lisp has built in.
The number of possible representations complicates the choice of a
good representation because semantically similar objects may vary
in their efficiency depending on how the program operates on
them.<br>
<br>
<a name="toc188" id="toc188"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc181" id="htoc181"><b><font size=
"4">5.10.1</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Think Before You
Use a List</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept215"></a><br>
Although Lisp's creator seemed to think that it was for LISt
Processing, the astute observer may have noticed that the chapter
on list manipulation makes up less that three percent of <i>Common
Lisp: The Language II</i>. The language has grown since Lisp
1.5---new data types supersede lists for many purposes.<br>
<br>
<a name="toc189" id="toc189"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc182" id="htoc182"><b><font size=
"4">5.10.2</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Structure
Representation</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept216"></a> One of the best ways of building complex
data structures is to define appropriate structure types using
<a name="@funs137"></a><tt class="code">defstruct</tt>. In Python,
access of structure slots is always at least as fast as list or
vector access, and is usually faster. In comparison to a list
representation of a tuple, structures also have a space
advantage.<br>
<br>
Even if structures weren't more efficient than other
representations, structure use would still be attractive because
programs that use structures in appropriate ways are much more
maintainable and robust than programs written using only lists. For
example:
<blockquote class="lisp">
<pre>
(rplaca (caddr (cadddr x)) (caddr y))
</pre></blockquote>
could have been written using structures in this way:
<blockquote class="lisp">
<pre>
(setf (beverage-flavor (astronaut-beverage x)) (beverage-flavor y))
</pre></blockquote>
The second version is more maintainable because it is easier to
understand what it is doing. It is more robust because structures
accesses are type checked. An <tt class="code">astronaut</tt> will
never be confused with a <tt class="code">beverage</tt>, and the
result of <tt class="code">beverage-flavor</tt> is always a flavor.
See sections <a href="#structure-types">5.2.8</a> and <a href=
"#freeze-type">5.2.9</a> for more information about structure
types. See section&nbsp;<a href="#type-inference">5.3</a> for a
number of examples that make clear the advantages of structure
typing.<br>
<br>
Note that the structure definition should be compiled before any
uses of its accessors or type predicate so that these function
calls can be efficiently open-coded.<br>
<br>
<a name="toc190" id="toc190"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc183" id="htoc183"><b><font size=
"4">5.10.3</font></b></a></td>
<td width="100%" align="center"><b><font size=
"4">Arrays</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="array-types" id="array-types"></a> <a name=
"@concept217"></a><br>
Arrays are often the most efficient representation for collections
of objects because:
<ul>
<li>Array representations are often the most compact. An array is
always more compact than a list containing the same number of
elements.<br>
<br></li>
<li>Arrays allow fast constant-time access.<br>
<br></li>
<li>Arrays are easily destructively modified, which can reduce
consing.<br>
<br></li>
<li>Array element types can be specialized, which reduces both
overall size and consing (see section&nbsp;<a href=
"#specialized-array-types">5.11.8</a>.)</li>
</ul>
Access of arrays that are not of type <tt class=
"code">simple-array</tt> is less efficient, so declarations are
appropriate when an array is of a simple type like <tt class=
"code">simple-string</tt> or <tt class=
"code">simple-bit-vector</tt>. Arrays are almost always simple, but
the compiler may not be able to prove simpleness at every use. The
only way to get a non-simple array is to use the <tt class=
"code">:displaced-to</tt>, <tt class="code">:fill-pointer</tt> or
<tt class="code">adjustable</tt> arguments to <tt class=
"code">make-array</tt>. If you don't use these hairy options, then
arrays can always be declared to be simple.<br>
<br>
Because of the many specialized array types and the possibility of
non-simple arrays, array access is much like generic arithmetic
(see section&nbsp;<a href="#generic-arithmetic">5.11.4</a>). In
order for array accesses to be efficiently compiled, the element
type and simpleness of the array must be known at compile time. If
there is inadequate information, the compiler is forced to call a
generic array access routine. You can detect inefficient array
accesses by enabling efficiency notes, see section&nbsp;<a href=
"#efficiency-notes">5.13</a>.<br>
<br>
<a name="toc191" id="toc191"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc184" id="htoc184"><b><font size=
"4">5.10.4</font></b></a></td>
<td width="100%" align="center"><b><font size=
"4">Vectors</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept218"></a><br>
Vectors (one dimensional arrays) are particularly useful, since in
addition to their obvious array-like applications, they are also
well suited to representing sequences. In comparison to a list
representation, vectors are faster to access and take up between
two and sixty-four times less space (depending on the element
type.) As with arbitrary arrays, the compiler needs to know that
vectors are not complex, so you should use <tt class=
"code">simple-string</tt> in preference to <tt class=
"code">string</tt>, etc.<br>
<br>
The only advantage that lists have over vectors for representing
sequences is that it is easy to change the length of a list, add to
it and remove items from it. Likely signs of archaic, slow lisp
code are <tt class="code">nth</tt> and <tt class=
"code">nthcdr</tt>. If you are using these functions you should
probably be using a vector.<br>
<br>
<a name="toc192" id="toc192"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc185" id="htoc185"><b><font size=
"4">5.10.5</font></b></a></td>
<td width="100%" align="center"><b><font size=
"4">Bit-Vectors</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept219"></a><br>
Another thing that lists have been used for is set manipulation. In
applications where there is a known, reasonably small universe of
items bit-vectors can be used to improve performance. This is much
less convenient than using lists, because instead of symbols, each
element in the universe must be assigned a numeric index into the
bit vector. Using a bit-vector will nearly always be faster, and
can be tremendously faster if the number of elements in the set is
not small. The logical operations on <tt class=
"code">simple-bit-vector</tt>s are efficient, since they operate on
a word at a time.<br>
<br>
<a name="toc193" id="toc193"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc186" id="htoc186"><b><font size=
"4">5.10.6</font></b></a></td>
<td width="100%" align="center"><b><font size=
"4">Hashtables</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept220"></a><br>
Hashtables are an efficient and general mechanism for maintaining
associations such as the association between an object and its
name. Although hashtables are usually the best way to maintain
associations, efficiency and style considerations sometimes favor
the use of an association list (a-list).<br>
<br>
<tt class="code">assoc</tt> is fairly fast when the <tt class=
"variable">test</tt> argument is <tt class="code">eq</tt> or
<tt class="code">eql</tt> and there are only a few elements, but
the time goes up in proportion with the number of elements. In
contrast, the hash-table lookup has a somewhat higher overhead, but
the speed is largely unaffected by the number of entries in the
table. For an <tt class="code">equal</tt> hash-table or alist,
hash-tables have an even greater advantage, since the test is more
expensive. Whatever you do, be sure to use the most restrictive
test function possible.<br>
<br>
The style argument observes that although hash-tables and alists
overlap in function, they do not do all things equally well.
<ul>
<li>Alists are good for maintaining scoped environments. They were
originally invented to implement scoping in the Lisp interpreter,
and are still used for this in Python. With an alist one can
non-destructively change an association simply by consing a new
element on the front. This is something that cannot be done with
hash-tables.<br>
<br></li>
<li>Hashtables are good for maintaining a global association. The
value associated with an entry can easily be changed with
<tt class="code">setf</tt>. With an alist, one has to go through
contortions, either <tt class="code">rplacd</tt>'ing the cons if
the entry exists, or pushing a new one if it doesn't. The
side-effecting nature of hash-table operations is an advantage
here.</li>
</ul>
Historically, symbol property lists were often used for global name
associations. Property lists provide an awkward and error-prone
combination of name association and record structure. If you must
use the property list, please store all the related values in a
single structure under a single property, rather than using many
properties. This makes access more efficient, and also adds a
modicum of typing and abstraction. See section&nbsp;<a href=
"#advanced-type-stuff">5.2</a> for information on types in
CMUCL.<br>
<br>
<a name="toc194" id="toc194"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFD3">
<div align="center">
<table>
<tr>
<td><a name="htoc187" id="htoc187"><b><font size=
"5">5.11</font></b></a></td>
<td width="100%" align="center"><b><font size=
"5">Numbers</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="numeric-types" id="numeric-types"></a> <a name=
"@concept221"></a> <a name="@concept222"></a><br>
Numbers are interesting because numbers are one of the few Common
Lisp data types that have direct support in conventional hardware.
If a number can be represented in the way that the hardware expects
it, then there is a big efficiency advantage.<br>
<br>
Using hardware representations is problematical in Common Lisp due
to dynamic typing (where the type of a value may be unknown at
compile time.) It is possible to compile code for statically typed
portions of a Common Lisp program with efficiency comparable to
that obtained in statically typed languages such as C, but not all
Common Lisp implementations succeed. There are two main barriers to
efficient numerical code in Common Lisp:
<ul>
<li>The compiler must prove that the numerical expression is in
fact statically typed, and<br>
<br></li>
<li>The compiler must be able to somehow reconcile the conflicting
demands of the hardware mandated number representation with the
Common Lisp requirements of dynamic typing and garbage-collecting
dynamic storage allocation.</li>
</ul>
Because of its type inference (see section&nbsp;<a href=
"#type-inference">5.3</a>) and efficiency notes (see
section&nbsp;<a href="#efficiency-notes">5.13</a>), Python is
better than conventional Common Lisp compilers at ensuring that
numerical expressions are statically typed. Python also goes
somewhat farther than existing compilers in the area of allowing
native machine number representations in the presence of garbage
collection.<br>
<br>
<a name="toc195" id="toc195"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc188" id="htoc188"><b><font size=
"4">5.11.1</font></b></a></td>
<td width="100%" align="center"><b><font size=
"4">Descriptors</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept223"></a> <a name="@concept224"></a> <a name=
"@concept225"></a> <a name="@concept226"></a><br>
Common Lisp's dynamic typing requires that it be possible to
represent any value with a fixed length object, known as a
<tt class="variable">descriptor</tt>. This fixed-length requirement
is implicit in features such as:
<ul>
<li>Data types (like <tt class="code">simple-vector</tt>) that can
contain any type of object, and that can be destructively modified
to contain different objects (of possibly different types.)<br>
<br></li>
<li>Functions that can be called with any type of argument, and
that can be redefined at run time.</li>
</ul>
In order to save space, a descriptor is invariably represented as a
single word. Objects that can be directly represented in the
descriptor itself are said to be <tt class=
"variable">immediate</tt>. Descriptors for objects larger than one
word are in reality pointers to the memory actually containing the
object.<br>
<br>
Representing objects using pointers has two major disadvantages:
<ul>
<li>The memory pointed to must be allocated on the heap, so it must
eventually be freed by the garbage collector. Excessive heap
allocation of objects (or ``consing'') is inefficient in several
ways. See section&nbsp;<a href="#consing">5.12.2</a>.<br>
<br></li>
<li>Representing an object in memory requires the compiler to emit
additional instructions to read the actual value in from memory,
and then to write the value back after operating on it.</li>
</ul>
The introduction of garbage collection makes things even worse,
since the garbage collector must be able to determine whether a
descriptor is an immediate object or a pointer. This requires that
a few bits in each descriptor be dedicated to the garbage
collector. The loss of a few bits doesn't seem like much, but it
has a major efficiency implication---objects whose natural machine
representation is a full word (integers and single-floats) cannot
have an immediate representation. So the compiler is forced to use
an unnatural immediate representation (such as <tt class=
"code">fixnum</tt>) or a natural pointer representation (with the
attendant consing overhead.)<br>
<br>
<a name="toc196" id="toc196"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc189" id="htoc189"><b><font size=
"4">5.11.2</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Non-Descriptor
Representations</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="non-descriptor" id="non-descriptor"></a> <a name=
"@concept227"></a> <a name="@concept228"></a><br>
From the discussion above, we can see that the standard descriptor
representation has many problems, the worst being number consing.
Common Lisp compilers try to avoid these descriptor efficiency
problems by using <tt class="variable">non-descriptor</tt>
representations. A compiler that uses non-descriptor
representations can compile this function so that it does no number
consing:
<blockquote class="lisp">
<pre>
(defun multby (vec n)
  (declare (type (simple-array single-float (*)) vec)
           (single-float n))
  (dotimes (i (length vec))
    (setf (aref vec i)
          (* n (aref vec i)))))
</pre></blockquote>
If a descriptor representation were used, each iteration of the
loop might cons two floats and do three times as many memory
references.<br>
<br>
As its negative definition suggests, the range of possible
non-descriptor representations is large. The performance
improvement from non-descriptor representation depends upon both
the number of types that have non-descriptor representations and
the number of contexts in which the compiler is forced to use a
descriptor representation.<br>
<br>
Many Common Lisp compilers support non-descriptor representations
for float types such as <tt class="code">single-float</tt> and
<tt class="code">double-float</tt> (section <a href=
"#float-efficiency">5.11.7</a>.) Python adds support for full word
integers (see section&nbsp;<a href="#word-integers">5.11.6</a>),
characters (see section&nbsp;<a href="#characters">5.11.11</a>) and
system-area pointers (unconstrained pointers, see
section&nbsp;<a href="unix.html#system-area-pointers">6.5</a>.)
Many Common Lisp compilers support non-descriptor representations
for variables (section <a href="#ND-variables">5.11.3</a>) and
array elements (section <a href=
"#specialized-array-types">5.11.8</a>.) Python adds support for
non-descriptor arguments and return values in local call (see
section&nbsp;<a href="#number-local-call">5.11.10</a>) and
structure slots (see section&nbsp;<a href=
"#raw-slots">5.11.9</a>).<br>
<br>
<a name="toc197" id="toc197"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc190" id="htoc190"><b><font size=
"4">5.11.3</font></b></a></td>
<td width="100%" align="center"><b><font size=
"4">Variables</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="ND-variables" id="ND-variables"></a> <a name=
"@concept229"></a> <a name="@concept230"></a> <a name=
"@concept231"></a><br>
In order to use a non-descriptor representation for a variable or
expression intermediate value, the compiler must be able to prove
that the value is always of a particular type having a
non-descriptor representation. Type inference (see
section&nbsp;<a href="#type-inference">5.3</a>) often needs some
help from user-supplied declarations. The best kind of type
declaration is a variable type declaration placed at the binding
point:
<blockquote class="lisp">
<pre>
(let ((x (car l)))
  (declare (single-float x))
  ...)
</pre></blockquote>
Use of <tt class="code">the</tt>, or of variable declarations not
at the binding form is insufficient to allow non-descriptor
representation of the variable---with these declarations it is not
certain that all values of the variable are of the right type. It
is sometimes useful to introduce a gratuitous binding that allows
the compiler to change to a non-descriptor representation, like:
<blockquote class="lisp">
<pre>
(etypecase x
  ((signed-byte 32)
   (let ((x x))
     (declare (type (signed-byte 32) x)) 
     ...))
  ...)
</pre></blockquote>
The declaration on the inner <tt class="code">x</tt> is necessary
here due to a phase ordering problem. Although the compiler will
eventually prove that the outer <tt class="code">x</tt> is a
<tt class="code">(signed-byte 32)</tt> within that <tt class=
"code">etypecase</tt> branch, the inner <tt class="code">x</tt>
would have been optimized away by that time. Declaring the type
makes let optimization more cautious.<br>
<br>
Note that storing a value into a global (or <tt class=
"code">special</tt>) variable always forces a descriptor
representation. Wherever possible, you should operate only on local
variables, binding any referenced globals to local variables at the
beginning of the function, and doing any global assignments at the
end.<br>
<br>
Efficiency notes signal use of inefficient representations, so
programmer's needn't continuously worry about the details of
representation selection (see section&nbsp;<a href=
"#representation-eff-note">5.13.3</a>.)<br>
<br>
<a name="toc198" id="toc198"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc191" id="htoc191"><b><font size=
"4">5.11.4</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Generic
Arithmetic</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="generic-arithmetic" id="generic-arithmetic"></a> <a name=
"@concept232"></a> <a name="@concept233"></a> <a name=
"@concept234"></a><br>
In Common Lisp, arithmetic operations are <tt class=
"variable">generic</tt>.<sup><a name="text13" href="#note13" id=
"text13"><font size="2">3</font></a></sup> The <tt class=
"code">+</tt> function can be passed <tt class="code">fixnum</tt>s,
<tt class="code">bignum</tt>s, <tt class="code">ratio</tt>s, and
various kinds of <tt class="code">float</tt>s and <tt class=
"code">complex</tt>es, in any combination. In addition to the
inherent complexity of <tt class="code">bignum</tt> and <tt class=
"code">ratio</tt> operations, there is also a lot of overhead in
just figuring out which operation to do and what contagion and
canonicalization rules apply. The complexity of generic arithmetic
is so great that it is inconceivable to open code it. Instead, the
compiler does a function call to a generic arithmetic routine,
consuming many instructions before the actual computation even
starts.<br>
<br>
This is ridiculous, since even Common Lisp programs do a lot of
arithmetic, and the hardware is capable of doing operations on
small integers and floats with a single instruction. To get
acceptable efficiency, the compiler special-cases uses of generic
arithmetic that are directly implemented in the hardware. In order
to open code arithmetic, several constraints must be met:
<ul>
<li>All the arguments must be known to be a good type of
number.<br>
<br></li>
<li>The result must be known to be a good type of number.<br>
<br></li>
<li>Any intermediate values such as the result of <tt class=
"code">(+ a b)</tt> in the call <tt class="code">(+ a b c)</tt>
must be known to be a good type of number.<br>
<br></li>
<li>All the above numbers with good types must be of the <tt class=
"variable">same</tt> good type. Don't try to mix integers and
floats or different float formats.</li>
</ul>
The ``good types'' are <tt class="code">(signed-byte 32)</tt>,
<tt class="code">(unsigned-byte 32)</tt>, <tt class=
"code">single-float</tt>, <tt class="code">double-float</tt>,
<tt class="code">(complex single-float)</tt>, and <tt class=
"code">(complex double-float)</tt>. See sections <a href=
"#fixnums">5.11.5</a>, <a href="#word-integers">5.11.6</a> and
<a href="#float-efficiency">5.11.7</a> for more discussion of good
numeric types.<br>
<br>
<tt class="code">float</tt> is not a good type, since it might mean
either <tt class="code">single-float</tt> or <tt class=
"code">double-float</tt>. <tt class="code">integer</tt> is not a
good type, since it might mean <tt class="code">bignum</tt>.
<tt class="code">rational</tt> is not a good type, since it might
mean <tt class="code">ratio</tt>. Note however that these types are
still useful in declarations, since type inference may be able to
strengthen a weak declaration into a good one, when it would be at
a loss if there was no declaration at all (see
section&nbsp;<a href="#type-inference">5.3</a>). The <tt class=
"code">integer</tt> and <tt class="code">unsigned-byte</tt> (or
non-negative integer) types are especially useful in this regard,
since they can often be strengthened to a good integer type.<br>
<br>
As noted above, CMUCL has support for <tt class="code">(complex
single-float)</tt> and <tt class="code">(complex
double-float)</tt>. These can be unboxed and, thus, are quite
efficient. However, arithmetic with complex types such as:
<blockquote class="lisp">
<pre>
(complex float)
(complex fixnum)
</pre></blockquote>
will be significantly slower than the good complex types but is
still faster than <tt class="code">bignum</tt> or <tt class=
"code">ratio</tt> arithmetic, since the implementation is much
simpler.<br>
<br>
Note: don't use <tt class="code">/</tt> to divide integers unless
you want the overhead of rational arithmetic. Use <tt class=
"code">truncate</tt> even when you know that the arguments divide
evenly.<br>
<br>
You don't need to remember all the rules for how to get open-coded
arithmetic, since efficiency notes will tell you when and where
there is a problem---see section&nbsp;<a href=
"#efficiency-notes">5.13</a>.<br>
<br>
<a name="toc199" id="toc199"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc192" id="htoc192"><b><font size=
"4">5.11.5</font></b></a></td>
<td width="100%" align="center"><b><font size=
"4">Fixnums</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="fixnums" id="fixnums"></a> <a name="@concept235"></a>
<a name="@concept236"></a><br>
A fixnum is a ``FIXed precision NUMber''. In modern Common Lisp
implementations, fixnums can be represented with an immediate
descriptor, so operating on fixnums requires no consing or memory
references. Clever choice of representations also allows some
arithmetic operations to be done on fixnums using hardware
supported word-integer instructions, somewhat reducing the speed
penalty for using an unnatural integer representation.<br>
<br>
It is useful to distinguish the <tt class="code">fixnum</tt> type
from the fixnum representation of integers. In Python, there is
absolutely nothing magical about the <tt class="code">fixnum</tt>
type in comparison to other finite integer types. <tt class=
"code">fixnum</tt> is equivalent to (is defined with <tt class=
"code">deftype</tt> to be) <tt class="code">(signed-byte 30)</tt>.
<tt class="code">fixnum</tt> is simply the largest subset of
integers that <em>can be represented</em> using an immediate fixnum
descriptor.<br>
<br>
Unlike in other Common Lisp compilers, it is in no way desirable to
use the <tt class="code">fixnum</tt> type in declarations in
preference to more restrictive integer types such as <tt class=
"code">bit</tt>, <tt class="code">(integer -43 7)</tt> and
<tt class="code">(unsigned-byte 8)</tt>. Since Python does
understand these integer types, it is preferable to use the more
restrictive type, as it allows better type inference (see
section&nbsp;<a href="#operation-type-inference">5.3.4</a>.)<br>
<br>
The small, efficient fixnum is contrasted with bignum, or ``BIG
NUMber''. This is another descriptor representation for integers,
but this time a pointer representation that allows for arbitrarily
large integers. Bignum operations are less efficient than fixnum
operations, both because of the consing and memory reference
overheads of a pointer descriptor, and also because of the inherent
complexity of extended precision arithmetic. While fixnum
operations can often be done with a single instruction, bignum
operations are so complex that they are always done using generic
arithmetic.<br>
<br>
A crucial point is that the compiler will use generic arithmetic if
it can't <tt class="variable">prove</tt> that all the arguments,
intermediate values, and results are fixnums. With bounded integer
types such as <tt class="code">fixnum</tt>, the result type proves
to be especially problematical, since these types are not closed
under common arithmetic operations such as <tt class="code">+</tt>,
<tt class="code">-</tt>, <tt class="code">*</tt> and <tt class=
"code">/</tt>. For example, <tt class="code">(1+ (the fixnum
x))</tt> does not necessarily evaluate to a <tt class=
"code">fixnum</tt>. Bignums were added to Common Lisp to get around
this problem, but they really just transform the correctness
problem ``if this add overflows, you will get the wrong answer'' to
the efficiency problem ``if this add <tt class=
"variable">might</tt> overflow then your program will run slowly
(because of generic arithmetic.)''<br>
<br>
There is just no getting around the fact that the hardware only
directly supports short integers. To get the most efficient open
coding, the compiler must be able to prove that the result is a
good integer type. This is an argument in favor of using more
restrictive integer types: <tt class="code">(1+ (the fixnum
x))</tt> may not always be a <tt class="code">fixnum</tt>, but
<tt class="code">(1+ (the (unsigned-byte 8) x))</tt> always is. Of
course, you can also assert the result type by putting in lots of
<tt class="code">the</tt> declarations and then compiling with
<tt class="code">safety</tt> <tt class="code">0</tt>.<br>
<br>
<a name="toc200" id="toc200"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc193" id="htoc193"><b><font size=
"4">5.11.6</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Word
Integers</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="word-integers" id="word-integers"></a> <a name=
"@concept237"></a><br>
Python is unique in its efficient implementation of arithmetic on
full-word integers through non-descriptor representations and open
coding. Arithmetic on any subtype of these types:
<blockquote class="lisp">
<pre>
(signed-byte 32)
(unsigned-byte 32)
</pre></blockquote>
is reasonably efficient, although subtypes of <tt class=
"code">fixnum</tt> remain somewhat more efficient.<br>
<br>
If a word integer must be represented as a descriptor, then the
<tt class="code">bignum</tt> representation is used, with its
associated consing overhead. The support for word integers in no
way changes the language semantics, it just makes arithmetic on
small bignums vastly more efficient. It is fine to do arithmetic
operations with mixed <tt class="code">fixnum</tt> and word integer
operands; just declare the most specific integer type you can, and
let the compiler decide what representation to use.<br>
<br>
In fact, to most users, the greatest advantage of word integer
arithmetic is that it effectively provides a few guard bits on the
fixnum representation. If there are missing assertions on
intermediate values in a fixnum expression, the intermediate
results can usually be proved to fit in a word. After the whole
expression is evaluated, there will often be a fixnum assertion on
the final result, allowing creation of a fixnum result without even
checking for overflow.<br>
<br>
The remarks in section <a href="#fixnums">5.11.5</a> about fixnum
result type also apply to word integers; you must be careful to
give the compiler enough information to prove that the result is
still a word integer. This time, though, when we blow out of word
integers we land in into generic bignum arithmetic, which is much
worse than sleazing from <tt class="code">fixnum</tt>s to word
integers. Note that mixing <tt class="code">(unsigned-byte 32)</tt>
arguments with arguments of any signed type (such as <tt class=
"code">fixnum</tt>) is a no-no, since the result might not be
unsigned.<br>
<br>
<a name="toc201" id="toc201"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc194" id="htoc194"><b><font size=
"4">5.11.7</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Floating Point
Efficiency</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="float-efficiency" id="float-efficiency"></a> <a name=
"@concept238"></a><br>
Arithmetic on objects of type <tt class="code">single-float</tt>
and <tt class="code">double-float</tt> is efficiently implemented
using non-descriptor representations and open coding. As for
integer arithmetic, the arguments must be known to be of the same
float type. Unlike for integer arithmetic, the results and
intermediate values usually take care of themselves due to the
rules of float contagion, i.e. <tt class="code">(1+ (the
single-float x))</tt> is always a <tt class=
"code">single-float</tt>.<br>
<br>
Although they are not specially implemented, <tt class=
"code">short-float</tt> and <tt class="code">long-float</tt> are
also acceptable in declarations, since they are synonyms for the
<tt class="code">single-float</tt> and <tt class=
"code">double-float</tt> types, respectively.<br>
<br>
In CMUCL, list-style float type specifiers such as <tt class=
"code">(single-float 0.0 1.0)</tt> will be used to good effect.<br>
<br>
For example, in this function,
<blockquote class="example">
<pre>
  (defun square (x)
    (declare (type (single-float 0f0 10f0)))
    (* x x))
</pre></blockquote>
Python can deduce that the return type of the function <tt class=
"code">square</tt> is <tt class="code">(single-float 0f0
100f0)</tt>.<br>
<br>
Many union types are also supported so that
<blockquote class="example">
<pre>
  (+ (the (or (integer 1 1) (integer 5 5)) x)
     (the (or (integer 10 10) (integer 20 20)) y))
</pre></blockquote>
has the inferred type <tt class="code">(or (integer 11 11) (integer
15 15) (integer 21 21) (integer 25 25))</tt>. This also works for
floating-point numbers. Member types are also supported.<br>
<br>
CMUCL can also infer types for many mathematical functions
including square root, exponential and logarithmic functions,
trignometric functions and their inverses, and hyperbolic functions
and their inverses. For numeric code, this can greatly enhance
efficiency by allowing the compiler to use specialized versions of
the functions instead of the generic versions. The greatest benefit
of this type inference is determining that the result of the
function is real-valued number instead of possibly being a
complex-valued number.<br>
<br>
For example, consider the function
<blockquote class="example">
<pre>
  (defun fun (x)
    (declare (type (single-float (0f0) 100f0) x))
    (values (sqrt x) (log x)))
</pre></blockquote>
With this declaration, the compiler can determine that the argument
to <tt class="code">sqrt</tt> and <tt class="code">log</tt> are
always non-negative so that the result is always a <tt class=
"code">single-float</tt>. In fact, the return type for this
function is derived to be <tt class="code">(values (single-float
0f0 10f0) (single-float * 2f0))</tt>.<br>
<br>
If the declaration were reduced to just <tt class="code">(declare
(single-float x))</tt>, the argument to <tt class="code">sqrt</tt>
and <tt class="code">log</tt> could be negative. This forces the
use of the generic versions of these functions because the result
could be a complex number.<br>
<br>
We note, however, that proper interval arithmetic is not fully
implemented in the compiler so the inferred types may be slightly
in error due to round-off errors. This round-off error could
accumulate to cause the compiler to erroneously deduce the result
type and cause code to be removed as being
unreachable.<sup><a name="text14" href="#note14" id=
"text14"><font size="2">4</font></a></sup>Thus, the declarations
should only be precise enough for the compiler to deduce that a
real-valued argument to a function would produce a real-valued
result. The efficiency notes (see section&nbsp;<a href=
"#representation-eff-note">5.13.3</a>) from the compiler will guide
you on what declarations might be useful.<br>
<br>
When a float must be represented as a descriptor, a pointer
representation is used, creating consing overhead. For this reason,
you should try to avoid situations (such as full call and
non-specialized data structures) that force a descriptor
representation. See sections <a href=
"#specialized-array-types">5.11.8</a>, <a href=
"#raw-slots">5.11.9</a> and <a href=
"#number-local-call">5.11.10</a>.<br>
<br>
See section&nbsp;<a href="extensions.html#ieee-float">2.1.2</a> for
information on the extensions to support IEEE floating point.<br>
<br>
<a name="toc202" id="toc202"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFED">
<div align="center">
<table>
<tr>
<td><b>5.11.7.1</b></td>
<td width="100%" align="center"><b>Signed Zeroes and Special
Functions</b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<br>
CMUCL supports IEEE signed zeroes. In typical usage, the signed
zeroes are not a problem and can be treated as an unsigned zero.
However, some of the special functions have branch points at zero,
so care must be taken.<br>
<br>
For example, suppose we have the function
<blockquote class="example">
<pre>
  (defun fun (x)
    (declare (type (single-float 0f0) x))
    (log x))
</pre></blockquote>
The derived result of the function is <tt class="code">(OR
SINGLE-FLOAT (COMPLEX SINGLE-FLOAT))</tt> because the declared
values for <tt class="code">x</tt> includes both -0.0 and 0.0 and
<tt class="code">(log -0.0)</tt> is actually a complex number.
Because of this, the generic complex log routine is used.<br>
<br>
If the declaration for <tt class="code">x</tt> were <tt class=
"code">(single-float (0f0))</tt> so +0.0 is not included or
<tt class="code">(or (single-float (0f0)) (member 0f0))</tt> so
+0.0 is include but not -0.0, the derived type would be <tt class=
"code">single-float</tt> for both cases. By declaring <tt class=
"code">x</tt> this way, the log can be implemented using a fast
real-valued log routine instead of the generic log routine.<br>
<br>
CMUCL implements the branch cuts and values given by
Kahan<sup><a name="text15" href="#note15" id="text15"><font size=
"2">5</font></a></sup>.<br>
<br>
<a name="toc203" id="toc203"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc195" id="htoc195"><b><font size=
"4">5.11.8</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Specialized
Arrays</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="specialized-array-types" id="specialized-array-types"></a>
<a name="@concept239"></a> <a name="@concept240"></a> <a name=
"@concept241"></a><br>
Common Lisp supports specialized array element types through the
<tt class="code">:element-type</tt> argument to <tt class=
"code">make-array</tt>. When an array has a specialized element
type, only elements of that type can be stored in the array. From
this restriction comes two major efficiency advantages:
<ul>
<li>A specialized array can save space by packing multiple elements
into a single word. For example, a <tt class="code">base-char</tt>
array can have 4 elements per word, and a <tt class="code">bit</tt>
array can have 32. This space-efficient representation is possible
because it is not necessary to separately indicate the type of each
element.<br>
<br></li>
<li>The elements in a specialized array can be given the same
non-descriptor representation as the one used in registers and on
the stack, eliminating the need for representation conversions when
reading and writing array elements. For objects with pointer
descriptor representations (such as floats and word integers) there
is also a substantial consing reduction because it is not necessary
to allocate a new object every time an array element is
modified.</li>
</ul>
These are the specialized element types currently supported:
<blockquote class="lisp">
<pre>
bit
(unsigned-byte 2)
(unsigned-byte 4)
(unsigned-byte 8)
(unsigned-byte 16)
(unsigned-byte 32)
(signed-byte 8)
(signed-byte 16)
(signed-byte 30)
(signed-byte 32)
base-character
single-float
double-float
(complex single-float)
(complex double-float)
</pre></blockquote>
Although a <tt class="code">simple-vector</tt> can hold any type of
object, <tt class="code">t</tt> should still be considered a
specialized array type, since arrays with element type <tt class=
"code">t</tt> are specialized to hold descriptors.<br>
<br>
When using non-descriptor representations, it is particularly
important to make sure that array accesses are open-coded, since in
addition to the generic operation overhead, efficiency is lost when
the array element is converted to a descriptor so that it can be
passed to (or from) the generic access routine. You can detect
inefficient array accesses by enabling efficiency notes, see
section&nbsp;<a href="#efficiency-notes">5.13</a>. See
section&nbsp;<a href="#array-types">5.10.3</a>.<br>
<br>
<a name="toc204" id="toc204"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc196" id="htoc196"><b><font size=
"4">5.11.9</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Specialized
Structure Slots</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="raw-slots" id="raw-slots"></a> <a name="@concept242"></a>
<a name="@concept243"></a><br>
Structure slots declared by the <tt class="code">:type</tt>
<tt class="code">defstruct</tt> slot option to have certain known
numeric types are also given non-descriptor representations. These
types (and subtypes of these types) are supported:
<blockquote class="lisp">
<pre>
(unsigned-byte 32)
single-float
double-float
(complex single-float)
(complex double-float)
</pre></blockquote>
The primary advantage of specialized slot representations is a
large reduction spurious memory allocation and access overhead of
programs that intensively use these types.<br>
<br>
<a name="toc205" id="toc205"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc197" id="htoc197"><b><font size=
"4">5.11.10</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Interactions With
Local Call</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="number-local-call" id="number-local-call"></a> <a name=
"@concept244"></a> <a name="@concept245"></a> <a name=
"@concept246"></a><br>
Local call has many advantages (see section&nbsp;<a href=
"#local-call">5.6</a>); one relevant to our discussion here is that
local call extends the usefulness of non-descriptor
representations. If the compiler knows from the argument type that
an argument has a non-descriptor representation, then the argument
will be passed in that representation. The easiest way to ensure
that the argument type is known at compile time is to always
declare the argument type in the called function, like:
<blockquote class="lisp">
<pre>
(defun 2+f (x)
  (declare (single-float x))
  (+ x 2.0))
</pre></blockquote>
The advantages of passing arguments and return values in a
non-descriptor representation are the same as for non-descriptor
representations in general: reduced consing and memory access (see
section&nbsp;<a href="#non-descriptor">5.11.2</a>.) This extends
the applicative programming styles discussed in section <a href=
"#local-call">5.6</a> to numeric code. Also, if source files are
kept reasonably small, block compilation can be used to reduce
number consing to a minimum.<br>
<br>
Note that non-descriptor return values can only be used with the
known return convention (section <a href=
"#local-call-return">5.6.5</a>.) If the compiler can't prove that a
function always returns the same number of values, then it must use
the unknown values return convention, which requires a descriptor
representation. Pay attention to the known return efficiency notes
to avoid number consing.<br>
<br>
<a name="toc206" id="toc206"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc198" id="htoc198"><b><font size=
"4">5.11.11</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Representation of
Characters</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="characters" id="characters"></a> <a name=
"@concept247"></a> <a name="@concept248"></a><br>
Python also uses a non-descriptor representation for characters
when convenient. This improves the efficiency of string
manipulation, but is otherwise pretty invisible; characters have an
immediate descriptor representation, so there is not a great
penalty for converting a character to a descriptor. Nonetheless, it
may sometimes be helpful to declare character-valued variables as
<tt class="code">base-character</tt>.<br>
<br>
<a name="toc207" id="toc207"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFD3">
<div align="center">
<table>
<tr>
<td><a name="htoc199" id="htoc199"><b><font size=
"5">5.12</font></b></a></td>
<td width="100%" align="center"><b><font size="5">General
Efficiency Hints</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="general-efficiency" id="general-efficiency"></a> <a name=
"@concept249"></a><br>
This section is a summary of various implementation costs and ways
to get around them. These hints are relatively unrelated to the use
of the Python compiler, and probably also apply to most other
Common Lisp implementations. In each section, there are references
to related in-depth discussion.<br>
<br>
<a name="toc208" id="toc208"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc200" id="htoc200"><b><font size=
"4">5.12.1</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Compile Your
Code</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept250"></a><br>
At this point, the advantages of compiling code relative to running
it interpreted probably need not be emphasized too much, but
remember that in CMUCL, compiled code typically runs hundreds of
times faster than interpreted code. Also, compiled (<tt class=
"code">fasl</tt>) files load significantly faster than source
files, so it is worthwhile compiling files which are loaded many
times, even if the speed of the functions in the file is
unimportant.<br>
<br>
Even disregarding the efficiency advantages, compiled code is as
good or better than interpreted code. Compiled code can be debugged
at the source level (see chapter <a href=
"debugger.html#debugger">3</a>), and compiled code does more error
checking. For these reasons, the interpreter should be regarded
mainly as an interactive command interpreter, rather than as a
programming language implementation.<br>
<br>
Do not be concerned about the performance of your program until you
see its speed compiled. Some techniques that make compiled code run
faster make interpreted code run slower.<br>
<br>
<a name="toc209" id="toc209"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc201" id="htoc201"><b><font size=
"4">5.12.2</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Avoid Unnecessary
Consing</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="consing" id="consing"></a> <a name="@concept251"></a>
<a name="@concept252"></a> <a name="@concept253"></a> <a name=
"@concept254"></a><br>
Consing is another name for allocation of storage, as done by the
<tt class="code">cons</tt> function (hence its name.) <tt class=
"code">cons</tt> is by no means the only function which conses---so
does <tt class="code">make-array</tt> and many other functions.
Arithmetic and function call can also have hidden consing
overheads. Consing hurts performance in the following ways:
<ul>
<li>Consing reduces memory access locality, increasing paging
activity.<br>
<br></li>
<li>Consing takes time just like anything else.<br>
<br></li>
<li>Any space allocated eventually needs to be reclaimed, either by
garbage collection or by starting a new <tt class="code">lisp</tt>
process.</li>
</ul>
Consing is not undiluted evil, since programs do things other than
consing, and appropriate consing can speed up the real work. It
would certainly save time to allocate a vector of intermediate
results that are reused hundreds of times. Also, if it is necessary
to copy a large data structure many times, it may be more efficient
to update the data structure non-destructively; this somewhat
increases update overhead, but makes copying trivial.<br>
<br>
Note that the remarks in section <a href=
"#efficiency-overview">5.1.5</a> about the importance of separating
tuning from coding also apply to consing overhead. The majority of
consing will be done by a small portion of the program. The consing
hot spots are even less predictable than the CPU hot spots, so
don't waste time and create bugs by doing unnecessary consing
optimization. During initial coding, avoid unnecessary side-effects
and cons where it is convenient. If profiling reveals a consing
problem, <tt class="variable">then</tt> go back and fix the hot
spots.<br>
<br>
See section&nbsp;<a href="#non-descriptor">5.11.2</a> for a
discussion of how to avoid number consing in Python.<br>
<br>
<a name="toc210" id="toc210"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc202" id="htoc202"><b><font size=
"4">5.12.3</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Complex Argument
Syntax</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept255"></a> <a name="@concept256"></a> <a name=
"@concept257"></a> <a name="@concept258"></a><br>
Common Lisp has very powerful argument passing mechanisms.
Unfortunately, two of the most powerful mechanisms, rest arguments
and keyword arguments, have a significant performance penalty:
<ul>
<li>With keyword arguments, the called function has to parse the
supplied keywords by iterating over them and checking them against
the desired keywords.<br>
<br></li>
<li>With rest arguments, the function must cons a list to hold the
arguments. If a function is called many times or with many
arguments, large amounts of memory will be allocated.</li>
</ul>
Although rest argument consing is worse than keyword parsing,
neither problem is serious unless thousands of calls are made to
such a function. The use of keyword arguments is strongly
encouraged in functions with many arguments or with interfaces that
are likely to be extended, and rest arguments are often natural in
user interface functions.<br>
<br>
Optional arguments have some efficiency advantage over keyword
arguments, but their syntactic clumsiness and lack of extensibility
has caused many Common Lisp programmers to abandon use of optionals
except in functions that have obviously simple and immutable
interfaces (such as <tt class="code">subseq</tt>), or in functions
that are only called in a few places. When defining an interface
function to be used by other programmers or users, use of only
required and keyword arguments is recommended.<br>
<br>
Parsing of <tt class="code">defmacro</tt> keyword and rest
arguments is done at compile time, so a macro can be used to
provide a convenient syntax with an efficient implementation. If
the macro-expanded form contains no keyword or rest arguments, then
it is perfectly acceptable in inner loops.<br>
<br>
Keyword argument parsing overhead can also be avoided by use of
inline expansion (see section&nbsp;<a href=
"#inline-expansion">5.8</a>) and block compilation (section
<a href="#block-compilation">5.7</a>.)<br>
<br>
Note: the compiler open-codes most heavily used system functions
which have keyword or rest arguments, so that no run-time overhead
is involved.<br>
<br>
<a name="toc211" id="toc211"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc203" id="htoc203"><b><font size=
"4">5.12.4</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Mapping and
Iteration</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept259"></a><br>
One of the traditional Common Lisp programming styles is a highly
applicative one, involving the use of mapping functions and many
lists to store intermediate results. To compute the sum of the
square-roots of a list of numbers, one might say:
<blockquote class="lisp">
<pre>
(apply #'+ (mapcar #'sqrt list-of-numbers))
</pre></blockquote>
This programming style is clear and elegant, but unfortunately
results in slow code. There are two reasons why:
<ul>
<li>The creation of lists of intermediate results causes much
consing (see <a href="#consing">5.12.2</a>).<br>
<br></li>
<li>Each level of application requires another scan down the list.
Thus, disregarding other effects, the above code would probably
take twice as long as a straightforward iterative version.</li>
</ul>
An example of an iterative version of the same code:
<blockquote class="lisp">
<pre>
(do ((num list-of-numbers (cdr num))
     (sum 0 (+ (sqrt (car num)) sum)))
    ((null num) sum))
</pre></blockquote>
See sections <a href="#variable-type-inference">5.3.1</a> and
<a href="#let-optimization">5.4.1</a> for a discussion of the
interactions of iteration constructs with type inference and
variable optimization. Also, section <a href=
"#local-tail-recursion">5.6.4</a> discusses an applicative style of
iteration.<br>
<br>
<a name="toc212" id="toc212"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc204" id="htoc204"><b><font size=
"4">5.12.5</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Trace Files and
Disassembly</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="trace-files" id="trace-files"></a> <a name=
"@concept260"></a> <a name="@concept261"></a> <a name=
"@concept262"></a> <a name="@concept263"></a> <a name=
"@concept264"></a> <a name="@concept265"></a><br>
In order to write efficient code, you need to know the relative
costs of different operations. The main reason why writing
efficient Common Lisp code is difficult is that there are so many
operations, and the costs of these operations vary in obscure
context-dependent ways. Although efficiency notes point out some
problem areas, the only way to ensure generation of the best code
is to look at the assembly code output.<br>
<br>
The <tt class="code">disassemble</tt> function is a convenient way
to get the assembly code for a function, but it can be very
difficult to interpret, since the correspondence with the original
source code is weak. A better (but more awkward) option is to use
the <tt class="code">:trace-file</tt> argument to <tt class=
"code">compile-file</tt> to generate a trace file.<br>
<br>
A trace file is a dump of the compiler's internal representations,
including annotated assembly code. Each component in the program
gets four pages in the trace file (separated by `` <tt class=
"code"><i>L</i></tt>''):
<ul>
<li>The implicit-continuation (or IR1) representation of the
optimized source. This is a dump of the flow graph representation
used for ``source level'' optimizations. As you will quickly
notice, it is not really very close to the source. This
representation is not very useful to even sophisticated users.<br>
<br></li>
<li>The Virtual Machine (VM, or IR2) representation of the program.
This dump represents the generated code as sequences of ``Virtual
OPerations'' (VOPs.) This representation is intermediate between
the source and the assembly code---each VOP corresponds fairly
directly to some primitive function or construct, but a given VOP
also has a fairly predictable instruction sequence. An operation
(such as <tt class="code">+</tt>) may have multiple implementations
with different cost and applicability. The choice of a particular
VOP such as <tt class="code">+/fixnum</tt> or <tt class=
"code">+/single-float</tt> represents this choice of
implementation. Once you are familiar with it, the VM
representation is probably the most useful for determining what
implementation has been used.<br>
<br></li>
<li>An assembly listing, annotated with the VOP responsible for
generating the instructions. This listing is useful for figuring
out what a VOP does and how it is implemented in a particular
context, but its large size makes it more difficult to read.<br>
<br></li>
<li>A disassembly of the generated code, which has all
pseudo-operations expanded out, but is not annotated with
VOPs.</li>
</ul>
Note that trace file generation takes much space and time, since
the trace file is tens of times larger than the source file. To
avoid huge confusing trace files and much wasted time, it is best
to separate the critical program portion into its own file and then
generate the trace file from this small file.<br>
<br>
<a name="toc213" id="toc213"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFD3">
<div align="center">
<table>
<tr>
<td><a name="htoc205" id="htoc205"><b><font size=
"5">5.13</font></b></a></td>
<td width="100%" align="center"><b><font size="5">Efficiency
Notes</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="efficiency-notes" id="efficiency-notes"></a> <a name=
"@concept266"></a> <a name="@concept267"></a> <a name=
"@concept268"></a><br>
Efficiency notes are messages that warn the user that the compiler
has chosen a relatively inefficient implementation for some
operation. Usually an efficiency note reflects the compiler's
desire for more type information. If the type of the values
concerned is known to the programmer, then additional declarations
can be used to get a more efficient implementation.<br>
<br>
Efficiency notes are controlled by the <tt class=
"code">extensions:inhibit-warnings</tt> (see section&nbsp;<a href=
"compiler.html#optimize-declaration">4.7.1</a>) optimization
quality. When <tt class="code">speed</tt> is greater than
<tt class="code">extensions:inhibit-warnings</tt>, efficiency notes
are enabled. Note that this implicitly enables efficiency notes
whenever <tt class="code">speed</tt> is increased from its default
of <tt class="code">1</tt>.<br>
<br>
Consider this program with an obscure missing declaration:
<blockquote class="lisp">
<pre>
(defun eff-note (x y z)
  (declare (fixnum x y z))
  (the fixnum (+ x y z)))
</pre></blockquote>
If compiled with <tt class="code">(speed 3) (safety 0)</tt>, this
note is given:
<blockquote class="example">
<pre>
In: DEFUN EFF-NOTE
  (+ X Y Z)
==&gt;
  (+ (+ X Y) Z)
Note: Forced to do inline (signed-byte 32) arithmetic (cost 3).
      Unable to do inline fixnum arithmetic (cost 2) because:
      The first argument is a (INTEGER -1073741824 1073741822),
      not a FIXNUM.
</pre></blockquote>
This efficiency note tells us that the result of the intermediate
computation <tt class="code">(+ x y)</tt> is not known to be a
<tt class="code">fixnum</tt>, so the addition of the intermediate
sum to <tt class="code">z</tt> must be done less efficiently. This
can be fixed by changing the definition of <tt class=
"code">eff-note</tt>:
<blockquote class="lisp">
<pre>
(defun eff-note (x y z)
  (declare (fixnum x y z))
  (the fixnum (+ (the fixnum (+ x y)) z)))
</pre></blockquote>
<a name="toc214" id="toc214"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc206" id="htoc206"><b><font size=
"4">5.13.1</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Type
Uncertainty</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept269"></a> <a name="@concept270"></a><br>
The main cause of inefficiency is the compiler's lack of adequate
information about the types of function argument and result values.
Many important operations (such as arithmetic) have an inefficient
general (generic) case, but have efficient implementations that can
usually be used if there is sufficient argument type
information.<br>
<br>
Type efficiency notes are given when a value's type is uncertain.
There is an important distinction between values that are <em>not
known</em> to be of a good type (uncertain) and values that are
<em>known not</em> to be of a good type. Efficiency notes are given
mainly for the first case (uncertain types.) If it is clear to the
compiler that that there is not an efficient implementation for a
particular function call, then an efficiency note will only be
given if the <tt class="code">extensions:inhibit-warnings</tt>
optimization quality is <tt class="code">0</tt> (see
section&nbsp;<a href=
"compiler.html#optimize-declaration">4.7.1</a>.)<br>
<br>
In other words, the default efficiency notes only suggest that you
add declarations, not that you change the semantics of your program
so that an efficient implementation will apply. For example,
compilation of this form will not give an efficiency note:
<blockquote class="lisp">
<pre>
(elt (the list l) i)
</pre></blockquote>
even though a vector access is more efficient than indexing a
list.<br>
<br>
<a name="toc215" id="toc215"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc207" id="htoc207"><b><font size=
"4">5.13.2</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Efficiency Notes
and Type Checking</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept271"></a> <a name="@concept272"></a> <a name=
"@concept273"></a><br>
It is important that the <tt class="code">eff-note</tt> example
above used <tt class="code">(safety 0)</tt>. When type checking is
enabled, you may get apparently spurious efficiency notes. With
<tt class="code">(safety 1)</tt>, the note has this extra line on
the end:
<blockquote class="example">
<pre>
The result is a (INTEGER -1610612736 1610612733), not a FIXNUM.
</pre></blockquote>
This seems strange, since there is a <tt class="code">the</tt>
declaration on the result of that second addition.<br>
<br>
In fact, the inefficiency is real, and is a consequence of Python's
treating declarations as assertions to be verified. The compiler
can't assume that the result type declaration is true---it must
generate the result and then test whether it is of the appropriate
type.<br>
<br>
In practice, this means that when you are tuning a program to run
without type checks, you should work from the efficiency notes
generated by unsafe compilation. If you want code to run
efficiently with type checking, then you should pay attention to
all the efficiency notes that you get during safe compilation.
Since user supplied output type assertions (e.g., from <tt class=
"code">the</tt>) are disregarded when selecting operation
implementations for safe code, you must somehow give the compiler
information that allows it to prove that the result truly must be
of a good type. In our example, it could be done by constraining
the argument types more:
<blockquote class="lisp">
<pre>
(defun eff-note (x y z)
  (declare (type (unsigned-byte 18) x y z))
  (+ x y z))
</pre></blockquote>
Of course, this declaration is acceptable only if the arguments to
<tt class="code">eff-note</tt> always <tt class="variable">are</tt>
<tt class="code">(unsigned-byte 18)</tt> integers.<br>
<br>
<a name="toc216" id="toc216"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc208" id="htoc208"><b><font size=
"4">5.13.3</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Representation
Efficiency Notes</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="representation-eff-note" id="representation-eff-note"></a>
<a name="@concept274"></a> <a name="@concept275"></a> <a name=
"@concept276"></a> <a name="@concept277"></a> <a name=
"@concept278"></a> <a name="@concept279"></a><br>
When operating on values that have non-descriptor representations
(see section&nbsp;<a href="#non-descriptor">5.11.2</a>), there can
be a substantial time and consing penalty for converting to and
from descriptor representations. For this reason, the compiler
gives an efficiency note whenever it is forced to do a
representation coercion more expensive than <a name=
"@vars53"></a><tt class=
"code">*efficiency-note-cost-threshold*</tt>.<br>
<br>
Inefficient representation coercions may be due to type
uncertainty, as in this example:
<blockquote class="lisp">
<pre>
(defun set-flo (x)
  (declare (single-float x))
  (prog ((var 0.0))
    (setq var (gorp))
    (setq var x)
    (return var)))
</pre></blockquote>
which produces this efficiency note:
<blockquote class="example">
<pre>
In: DEFUN SET-FLO
  (SETQ VAR X)
Note: Doing float to pointer coercion (cost 13) from X to VAR.
</pre></blockquote>
The variable <tt class="code">var</tt> is not known to always hold
values of type <tt class="code">single-float</tt>, so a descriptor
representation must be used for its value. In this sort of
situation, adding a declaration will eliminate the
inefficiency.<br>
<br>
Often inefficient representation conversions are not due to type
uncertainty---instead, they result from evaluating a non-descriptor
expression in a context that requires a descriptor result:
<ul>
<li>Assignment to or initialization of any data structure other
than a specialized array (see section&nbsp;<a href=
"#specialized-array-types">5.11.8</a>), or<br>
<br></li>
<li>Assignment to a <tt class="code">special</tt> variable, or<br>
<br></li>
<li>Passing as an argument or returning as a value in any function
call that is not a local call (see section&nbsp;<a href=
"#number-local-call">5.11.10</a>.)</li>
</ul>
If such inefficient coercions appear in a ``hot spot'' in the
program, data structures redesign or program reorganization may be
necessary to improve efficiency. See sections <a href=
"#block-compilation">5.7</a>, <a href="#numeric-types">5.11</a> and
<a href="#profiling">5.14</a>.<br>
<br>
Because representation selection is done rather late in
compilation, the source context in these efficiency notes is
somewhat vague, making interpretation more difficult. This is a
fairly straightforward example:
<blockquote class="lisp">
<pre>
(defun cf+ (x y)
  (declare (single-float x y))
  (cons (+ x y) t))
</pre></blockquote>
which gives this efficiency note:
<blockquote class="example">
<pre>
In: DEFUN CF+
  (CONS (+ X Y) T)
Note: Doing float to pointer coercion (cost 13), for:
      The first argument of CONS.
</pre></blockquote>
The source context form is almost always the form that receives the
value being coerced (as it is in the preceding example), but can
also be the source form which generates the coerced value.
Compiling this example:
<blockquote class="lisp">
<pre>
(defun if-cf+ (x y)
  (declare (single-float x y))
  (cons (if (grue) (+ x y) (snoc)) t))
</pre></blockquote>
produces this note:
<blockquote class="example">
<pre>
In: DEFUN IF-CF+
  (+ X Y)
Note: Doing float to pointer coercion (cost 13).
</pre></blockquote>
In either case, the note's text explanation attempts to include
additional information about what locations are the source and
destination of the coercion. Here are some example notes:
<blockquote class="example">
<pre>
  (IF (GRUE) X (SNOC))
Note: Doing float to pointer coercion (cost 13) from X.

  (SETQ VAR X)
Note: Doing float to pointer coercion (cost 13) from X to VAR.
</pre></blockquote>
Note that the return value of a function is also a place to which
coercions may have to be done:
<blockquote class="example">
<pre>
  (DEFUN F+ (X Y) (DECLARE (SINGLE-FLOAT X Y)) (+ X Y))
Note: Doing float to pointer coercion (cost 13) to "&lt;return value&gt;".
</pre></blockquote>
Sometimes the compiler is unable to determine a name for the source
or destination, in which case the source context is the only
clue.<br>
<br>
<a name="toc217" id="toc217"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc209" id="htoc209"><b><font size=
"4">5.13.4</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Verbosity
Control</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept280"></a> <a name="@concept281"></a><br>
These variables control the verbosity of efficiency notes:<br>
<br>
<br>
<a name="@vars54"></a><a name="VR:efficiency-note-cost-threshold"
id="VR:efficiency-note-cost-threshold"></a>
<div align="left">[Variable]<br>
<tt class="function-name">*efficiency-note-cost-threshold*</tt>
&nbsp;&nbsp;&nbsp;</div>
<blockquote><br>
Before printing some efficiency notes, the compiler compares the
value of this variable to the difference in cost between the chosen
implementation and the best potential implementation. If the
difference is not greater than this limit, then no note is printed.
The units are implementation dependent; the initial value
suppresses notes about ``trivial'' inefficiencies. A value of
<tt class="code">1</tt> will note any inefficiency.</blockquote>
<br>
<a name="@vars55"></a><a name="VR:efficiency-note-limit" id=
"VR:efficiency-note-limit"></a>
<div align="left">[Variable]<br>
<tt class="function-name">*efficiency-note-limit*</tt>
&nbsp;&nbsp;&nbsp;</div>
<blockquote><br>
When printing some efficiency notes, the compiler reports possible
efficient implementations. The initial value of <tt class=
"code">2</tt> prevents excessively long efficiency notes in the
common case where there is no type information, so all
implementations are possible.</blockquote>
<a name="toc218" id="toc218"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFD3">
<div align="center">
<table>
<tr>
<td><a name="htoc210" id="htoc210"><b><font size=
"5">5.14</font></b></a></td>
<td width="100%" align="center"><b><font size=
"5">Profiling</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept282"></a> <a name="@concept283"></a> <a name=
"@concept284"></a> <a name="@concept285"></a> <a name="profiling"
id="profiling"></a><br>
The first step in improving a program's performance is to profile
the activity of the program to find where it spends its time. The
best way to do this is to use the profiling utility found in the
<tt class="code">profile</tt> package. This package provides a
macro <tt class="code">profile</tt> that encapsulates functions
with statistics gathering code.<br>
<br>
<a name="toc219" id="toc219"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc211" id="htoc211"><b><font size=
"4">5.14.1</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Profile
Interface</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<br>
<br>
<a name="@vars56"></a><a name="VR:timed-functions" id=
"VR:timed-functions"></a>
<div align="left">[Variable]<br>
<tt class="function-name">profile:</tt><tt class=
"function-name">*timed-functions*</tt> &nbsp;&nbsp;&nbsp;</div>
<blockquote><br>
This variable holds a list of all functions that are currently
being profiled.</blockquote>
<br>
<a name="@funs138"></a><a name="FN:profile" id="FN:profile"></a>
<div align="left">[Macro]<br>
<tt class="function-name">profile:</tt><tt class=
"function-name">profile</tt> <tt class="code">{<tt class=
"variable">name</tt> |<tt class="code">:callers</tt> <tt class=
"code">t</tt>}</tt><sup><font size="2">*</font></sup>
&nbsp;&nbsp;&nbsp;</div>
<blockquote><br>
This macro wraps profiling code around the named functions. As in
<tt class="code">trace</tt>, the <tt class="variable">name</tt>s
are not evaluated. If a function is already profiled, then the
function is unprofiled and reprofiled (useful to notice function
redefinition.) A warning is printed for each name that is not a
defined function.<br>
<br>
If <tt class="code">:callers <tt class="variable">t</tt></tt> is
specified, then each function that calls this function is recorded
along with the number of calls made.</blockquote>
<br>
<a name="@funs139"></a><a name="FN:unprofile" id=
"FN:unprofile"></a>
<div align="left">[Macro]<br>
<tt class="function-name">profile:</tt><tt class=
"function-name">unprofile</tt> <tt class="code">{<tt class=
"variable">name</tt>}</tt><sup><font size="2">*</font></sup>
&nbsp;&nbsp;&nbsp;</div>
<blockquote><br>
This macro removes profiling code from the named functions. If no
<tt class="variable">name</tt>s are supplied, all currently
profiled functions are unprofiled.</blockquote>
<br>
<a name="@funs140"></a><a name="FN:profile-all" id=
"FN:profile-all"></a>
<div align="left">[Macro]<br>
<tt class="function-name">profile:</tt><tt class=
"function-name">profile-all</tt> <tt class="code">&amp;key</tt>
<tt class="code">:package</tt> <tt class="code">:callers-p</tt>
&nbsp;&nbsp;&nbsp;</div>
<blockquote><br>
This macro in effect calls <tt class="code">profile:profile</tt>
for each function in the specified package which defaults to
<tt class="code">*package*</tt>. <tt class="code">:callers-p</tt>
has the same meaning as in <tt class=
"code">profile:profile</tt>.</blockquote>
<br>
<a name="@funs141"></a><a name="FN:report-time" id=
"FN:report-time"></a>
<div align="left">[Macro]<br>
<tt class="function-name">profile:</tt><tt class=
"function-name">report-time</tt> <tt class="code">{<tt class=
"variable">name</tt>}</tt><sup><font size="2">*</font></sup>
&nbsp;&nbsp;&nbsp;</div>
<blockquote><br>
This macro prints a report for each <tt class="variable">name</tt>d
function of the following information:
<ul>
<li>The total CPU time used in that function for all calls,<br>
<br></li>
<li>the total number of bytes consed in that function for all
calls,<br>
<br></li>
<li>the total number of calls,<br>
<br></li>
<li>the average amount of CPU time per call.</li>
</ul>
Summary totals of the CPU time, consing and calls columns are
printed. An estimate of the profiling overhead is also printed (see
below). If no <tt class="variable">name</tt>s are supplied, then
the times for all currently profiled functions are
printed.</blockquote>
<br>
<a name="@funs142"></a><a name="FN:reset-time" id=
"FN:reset-time"></a>
<div align="left">[Macro]<br>
<tt class="function-name">reset-time</tt> <tt class=
"code">{<tt class="variable">name</tt>}</tt><sup><font size=
"2">*</font></sup> &nbsp;&nbsp;&nbsp;</div>
<blockquote><br>
This macro resets the profiling counters associated with the
<tt class="variable">name</tt>d functions. If no <tt class=
"variable">name</tt>s are supplied, then all currently profiled
functions are reset.</blockquote>
<a name="toc220" id="toc220"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc212" id="htoc212"><b><font size=
"4">5.14.2</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Profiling
Techniques</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<br>
Start by profiling big pieces of a program, then carefully choose
which functions close to, but not in, the inner loop are to be
profiled next. Avoid profiling functions that are called by other
profiled functions, since this opens the possibility of profiling
overhead being included in the reported times.<br>
<br>
If the per-call time reported is less than 1/10 second, then
consider the clock resolution and profiling overhead before you
believe the time. It may be that you will need to run your program
many times in order to average out to a higher resolution.<br>
<br>
<a name="toc221" id="toc221"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc213" id="htoc213"><b><font size=
"4">5.14.3</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Nested or
Recursive Calls</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<br>
The profiler attempts to compensate for nested or recursive calls.
Time and consing overhead will be charged to the dynamically
innermost (most recent) call to a profiled function. So profiling a
subfunction of a profiled function will cause the reported time for
the outer function to decrease. However if an inner function has a
large number of calls, some of the profiling overhead may ``leak''
into the reported time for the outer function. In general, be wary
of profiling short functions that are called many times.<br>
<br>
<a name="toc222" id="toc222"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc214" id="htoc214"><b><font size=
"4">5.14.4</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Clock
resolution</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<br>
Unless you are very lucky, the length of your machine's clock
``tick'' is probably much longer than the time it takes simple
function to run. For example, on the IBM RT, the clock resolution
is 1/50 second. This means that if a function is only called a few
times, then only the first couple decimal places are really
meaningful.<br>
<br>
Note however, that if a function is called many times, then the
statistical averaging across all calls should result in increased
resolution. For example, on the IBM RT, if a function is called a
thousand times, then a resolution of tens of microseconds can be
expected.<br>
<br>
<a name="toc223" id="toc223"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc215" id="htoc215"><b><font size=
"4">5.14.5</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Profiling
overhead</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<br>
The added profiling code takes time to run every time that the
profiled function is called, which can disrupt the attempt to
collect timing information. In order to avoid serious inflation of
the times for functions that take little time to run, an estimate
of the overhead due to profiling is subtracted from the times
reported for each function.<br>
<br>
Although this correction works fairly well, it is not totally
accurate, resulting in times that become increasingly meaningless
for functions with short runtimes. This is only a concern when the
estimated profiling overhead is many times larger than reported
total CPU time.<br>
<br>
The estimated profiling overhead is not represented in the reported
total CPU time. The sum of total CPU time and the estimated
profiling overhead should be close to the total CPU time for the
entire profiling run (as determined by the <tt class=
"code">time</tt> macro.) Time unaccounted for is probably being
used by functions that you forgot to profile.<br>
<br>
<a name="toc224" id="toc224"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc216" id="htoc216"><b><font size=
"4">5.14.6</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Additional Timing
Utilities</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<br>
<br>
<a name="@funs143"></a><a name="FN:time" id="FN:time"></a>
<div align="left">[Macro]<br>
<tt class="function-name">time</tt> <tt class="variable">form</tt>
&nbsp;&nbsp;&nbsp;</div>
<blockquote><br>
This macro evaluates <tt class="variable">form</tt>, prints some
timing and memory allocation information to <tt class=
"code">*trace-output*</tt>, and returns any values that <tt class=
"variable">form</tt> returns. The timing information includes real
time, user run time, and system run time. This macro executes a
form and reports the time and consing overhead. If the <tt class=
"code">time</tt> form is not compiled (e.g. it was typed at
top-level), then <tt class="code">compile</tt> will be called on
the form to give more accurate timing information. If you really
want to time interpreted speed, you can say:
<blockquote class="lisp">
<pre>
(time (eval '<tt class="variable">form</tt>))
</pre></blockquote>
Things that execute fairly quickly should be timed more than once,
since there may be more paging overhead in the first timing. To
increase the accuracy of very short times, you can time multiple
evaluations:
<blockquote class="lisp">
<pre>
(time (dotimes (i 100) <tt class="variable">form</tt>))
</pre></blockquote>
</blockquote>
<br>
<a name="@funs144"></a><a name="FN:get-bytes-consed" id=
"FN:get-bytes-consed"></a>
<div align="left">[Function]<br>
<tt class="function-name">extensions:</tt><tt class=
"function-name">get-bytes-consed</tt> &nbsp;&nbsp;&nbsp;</div>
<blockquote><br>
This function returns the number of bytes allocated since the first
time you called it. The first time it is called it returns zero.
The above profiling routines use this to report consing
information.</blockquote>
<br>
<a name="@vars57"></a><a name="VR:gc-run-time" id=
"VR:gc-run-time"></a>
<div align="left">[Variable]<br>
<tt class="function-name">extensions:</tt><tt class=
"function-name">*gc-run-time*</tt> &nbsp;&nbsp;&nbsp;</div>
<blockquote><br>
This variable accumulates the run-time consumed by garbage
collection, in the units returned by <a name=
"@funs145"></a><tt class=
"code">get-internal-run-time</tt>.</blockquote>
<br>
<div align="left">[Constant]<br>
<tt class="function-name">internal-time-units-per-second</tt>
&nbsp;&nbsp;&nbsp;</div>
<blockquote>The value of internal-time-units-per-second is
100.</blockquote>
<a name="toc225" id="toc225"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc217" id="htoc217"><b><font size=
"4">5.14.7</font></b></a></td>
<td width="100%" align="center"><b><font size="4">A Note on
Timing</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept286"></a> <a name="@concept287"></a> <a name=
"@concept288"></a><br>
There are two general kinds of timing information provided by the
<tt class="code">time</tt> macro and other profiling utilities:
real time and run time. Real time is elapsed, wall clock time. It
will be affected in a fairly obvious way by any other activity on
the machine. The more other processes contending for CPU and
memory, the more real time will increase. This means that real time
measurements are difficult to replicate, though this is less true
on a dedicated workstation. The advantage of real time is that it
is real. It tells you really how long the program took to run under
the benchmarking conditions. The problem is that you don't know
exactly what those conditions were.<br>
<br>
Run time is the amount of time that the processor supposedly spent
running the program, as opposed to waiting for I/O or running other
processes. ``User run time'' and ``system run time'' are numbers
reported by the Unix kernel. They are supposed to be a measure of
how much time the processor spent running your ``user'' program
(which will include GC overhead, etc.), and the amount of time that
the kernel spent running ``on your behalf.''<br>
<br>
Ideally, user time should be totally unaffected by benchmarking
conditions; in reality user time does depend on other system
activity, though in rather non-obvious ways.<br>
<br>
System time will clearly depend on benchmarking conditions. In Lisp
benchmarking, paging activity increases system run time (but not by
as much as it increases real time, since the kernel spends some
time waiting for the disk, and this is not run time, kernel or
otherwise.)<br>
<br>
In my experience, the biggest trap in interpreting kernel/user run
time is to look only at user time. In reality, it seems that the
<tt class="variable">sum</tt> of kernel and user time is more
reproducible. The problem is that as system activity increases,
there is a spurious <tt class="variable">decrease</tt> in user run
time. In effect, as paging, etc., increases, user time leaks into
system time.<br>
<br>
So, in practice, the only way to get truly reproducible results is
to run with the same competing activity on the system. Try to run
on a machine with nobody else logged in, and check with ``ps aux''
to see if there are any system processes munching large amounts of
CPU or memory. If the ratio between real time and the sum of user
and system time varies much between runs, then you have a
problem.<br>
<br>
<a name="toc226" id="toc226"></a>
<table cellpadding="0" cellspacing="0" width="100%">
<tr>
<td bgcolor="#FFFFE2">
<div align="center">
<table>
<tr>
<td><a name="htoc218" id="htoc218"><b><font size=
"4">5.14.8</font></b></a></td>
<td width="100%" align="center"><b><font size="4">Benchmarking
Techniques</font></b></td>
</tr>
</table>
</div>
</td>
</tr>
</table>
<a name="@concept289"></a><br>
Given these imperfect timing tools, how do should you do
benchmarking? The answer depends on whether you are trying to
measure improvements in the performance of a single program on the
same hardware, or if you are trying to compare the performance of
different programs and/or different hardware.<br>
<br>
For the first use (measuring the effect of program modifications
with constant hardware), you should look at <tt class=
"variable">both</tt> system+user and real time to understand what
effect the change had on CPU use, and on I/O (including paging.) If
you are working on a CPU intensive program, the change in
system+user time will give you a moderately reproducible measure of
performance across a fairly wide range of system conditions. For a
CPU intensive program, you can think of system+user as ``how long
it would have taken to run if I had my own machine.'' So in the
case of comparing CPU intensive programs, system+user time is
relatively real, and reasonable to use.<br>
<br>
For programs that spend a substantial amount of their time paging,
you really can't predict elapsed time under a given operating
condition without benchmarking in that condition. User or
system+user time may be fairly reproducible, but it is also
relatively meaningless, since in a paging or I/O intensive program,
the program is spending its time waiting, not running, and system
time and user time are both measures of run time. A change that
reduces run time might increase real time by increasing paging.<br>
<br>
Another common use for benchmarking is comparing the performance of
the same program on different hardware. You want to know which
machine to run your program on. For comparing different machines
(operating systems, etc.), the only way to compare that makes sense
is to set up the machines in <tt class="variable">exactly</tt> the
way that they will <tt class="variable">normally</tt> be run, and
then measure <tt class="variable">real</tt> time. If the program
will normally be run along with X, then run X. If the program will
normally be run on a dedicated workstation, then be sure nobody
else is on the benchmarking machine. If the program will normally
be run on a machine with three other Lisp jobs, then run three
other Lisp jobs. If the program will normally be run on a machine
with 64MB of memory, then run with 64MB. Here, ``normal'' means
``normal for that machine''.<br>
<br>
If you have a program you believe to be CPU intensive, then you
might be tempted to compare ``run'' times across systems, hoping to
get a meaningful result even if the benchmarking isn't done under
the expected running condition. Don't to this, for two reasons:
<ul>
<li>The operating systems might not compute run time in the same
way.<br>
<br></li>
<li>Under the real running condition, the program might not be CPU
intensive after all.</li>
</ul>
In the end, only real time means anything---it is the amount of
time you have to wait for the result. The only valid uses for run
time are:
<ul>
<li>To develop insight into the program. For example, if run time
is much less than elapsed time, then you are probably spending lots
of time paging.<br>
<br></li>
<li>To evaluate the relative performance of CPU intensive programs
in the same environment.</li>
</ul>
<hr width="50%" size="1">
<dl>
<dt><a name="note11" href="#text11" id="note11"><font size=
"5">1</font></a></dt>
<dd>The source transformation in this example doesn't represent the
preservation of evaluation order implicit in the compiler's
internal representation. Where necessary, the back end will
reintroduce temporaries to preserve the semantics.</dd>
<dt><a name="note12" href="#text12" id="note12"><font size=
"5">2</font></a></dt>
<dd>Note that the code for <tt class="code">x</tt> and <tt class=
"code">y</tt> isn't actually replicated.</dd>
<dt><a name="note13" href="#text13" id="note13"><font size=
"5">3</font></a></dt>
<dd>As Steele notes in CLTL II, this is a generic conception of
generic, and is not to be confused with the CLOS concept of a
generic function.</dd>
<dt><a name="note14" href="#text14" id="note14"><font size=
"5">4</font></a></dt>
<dd>This, however, has not actually happened, but it is a
possibility.</dd>
<dt><a name="note15" href="#text15" id="note15"><font size=
"5">5</font></a></dt>
<dd>Kahan, W., ``Branch Cuts for Complex Elementary Functions, or
Much Ado About Nothing's Sign Bit'' in Iserles and Powell (eds.)
<i>The State of the Art in Numerical Analysis</i>, pp. 165-211,
Clarendon Press, 1987</dd>
</dl>
<hr>
<a href="compiler.html"><img src="previous_motif.gif" alt=
"Previous"></a> <a href="index.html"><img src="contents_motif.gif"
alt="Up"></a> <a href="unix.html"><img src="next_motif.gif" alt=
"Next"></a>
</body>
</html>
